## Regularization AKA how to fix overfitting and collinearity {#regularization}

Now, if you were to consult other machine learning texts, you will notice one word that often comes up... regularization. Regularization only applies to linear models because it is predicated on changing the slope of a fitted line to reduce overfitting issues. For our purposes, we will not be able to use regularization in this tutorial. This is because linear regression is not a classification based model, the assumptions weren't met for logistic regression, and tuning regularization with support vector machines is outside the scope of this tutorial. If you have no interest in regularization you can skip this section. I’ve included this section in the tutorial because it’s a key concept in machine learning, commonly applied whenever linear or logistic regression is used.

Regularization helps us to solve two problems that plague statistics: collinearity and overfitting. Often after you collect your data, you will find that some of your predictors are [colinear](https://www.stratascratch.com/blog/a-beginner-s-guide-to-collinearity-what-it-is-and-how-it-affects-our-regression-model/), meaning that two or more of your predictor variables are highly correlated with each other. [Overfitting](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/) means that your model too closely maps onto your training data and isn't flexible enough to do well or make accurate predictions for new unseen data. All data represents two sources of variation: the true relationships, and the error in your sample (i.e., what is true of your sample but is not true of the population). The smaller your sample size, the more likely it is that you have captured noise rather than the true relationship between variables. The graphic below helps illustrate how overfitting creates a model reliant on error rather than the true relationship.

![overfitting](https://www.mathworks.com/discovery/overfitting/_jcr_content/mainParsys/image.adapt.full.medium.svg/1705396624275.svg)

You can solve both of these issues at once with something called regularization. Regularization simply adds a penalty term, lambda, to the slope of the fitted line. This additional value added to the slope adds in a little bit of error to the model. This user-added error is a necessary evil to combat some of the noise that occur in models when the data used to construct the model is not totally representative of the underlying truth of the relationship between variables. The choice to regularize is totally up to you, and is based on how much risk tolerance or uncertainty you want to include in your model outcomes. If you have millions of data points from a highly representative sample, adding in that extra error will probably get you further from the truth of the relationship. If you have, like we do in this example, less than 1000 subjects and are trying to generalize to the species level you probably should do regularization as the sample is small and not totally representative of your total population.

There are three types of regularization that are commonly used: 

1. Lasso Regularization - best to use if you know some of the predictors in your model are useless
2. Ridge Regularization - best to use if you know all of the predictors in your model are useful
3. Elastic Net Regularization - best to use if you don't know how useful/useless your predictors are because it has the best of both worlds ([Chill it out take it slow than you'll rock out the show](https://www.youtube.com/watch?v=uVjRe8QXFHY)) - it does the math from lasso + math from ridge to get a new number that is called elastic net.

In each case, our algorithm creates a value that adds together the loss/cost function, a lambda value, and  then multiples this number by a transformation of the beta weights. How the beta weights are transformed is where the three types differ. If you got a little lost in that last sentence, don't despair; we will describe each of the three parts that contribute to regularization in detail. I'll clarify this concept using an example you are likely familiar with, linear regression. 

In a linear regression model, our loss/cost function is the sum of squared residuals. The sum of squared residuals (calculated by adding together all the squared values of the differences between each data point and the estimate of where the line says the data point should be) is a value that represents how well the line did at predicting actual values. 

The second term, Lambda, can be any positive number. The larger the lambda, the larger the penalty or error you are adding to the model. The lambda is calculated by iteratively going thru various values of lambda until you find one that minimizes the regularization value. 

The difference between the three types of regularization occur in the third term, which represents how they transform the beta weights of each variable. In Lasso regularization, you multiply by the absolute value of the beta weights (i.e., |slope|). In Ridge regularization you multiply by the squared beta weights (i.e., slope^2^). In elastic net, you multiply by adding together both penalties (i.e., |slope| + slope^2^).

You use Ridge regularization when you believe all your variables are useful, and Lasso when you suspect some are not, due to the difference between squaring values in Ridge and using absolute values in Lasso. Essentially, it is possible to set a predictor to exactly 0 when you take the absolute value, but when you square a term it can only ever get very small but never exactly 0. Because of this, Lasso and Elastic Net regularization can set some predictor beta weights to exactly 0, eliminating those predictors from the model. On the other hand, Ridge will always have a beta weight value for each predictor. The smaller the beta weight for Ridge regularization the less predictive an individual predictor will be to the model. In this way Lasso and Elastic net regularization allows you to have math take out useless predictors instead of doing it in some semi-arbitrary way (NICE!).

Two really important limitations of regularization are:

1. When working with collinear variables, the way the math works behind the scenes, Lasso and Elastic Net will always set the first collinear variable to 0 and keep the second variable, so be mindful of this when you order the model variables. In practical terms, this means that you should enter terms into the model in order of importance based on domain knowledge. 
2. These regularization techniques only works with models where the relationship between predictors and the outcome is linear, or the model parameters are linear. Of the methods mentioned in this tutorial, the only algorithms that regularization works are linear regression, logistic regression, and support vector machines.

For our purposes, we will not be able to use regularization in this tutorial. This is because linear regression is not a classification based model, the assumptions weren't met for logistic regression, and tuning regularization with support vector machines is outside the scope of this tutorial. However, if we were to run regularization for these questions, we would choose ridge regularization for research questions 1 and 2, as there is only one predictor variable (meaning it would need to be predictive for there to be a model!). For research question 3, where we are interested in which predictor variables out of many are important, we would use Elastic Net regularization because we want the algorithm to help us decide which predictors to use in the model.

colinearity: bascially reducse the btea to 0 removing it from the model thus eliminating the possitivbility or colinearity to occur but it did that without having to choose it did it with iteratize learning. 

for example, we have a model with two useful predictors nad one useless predictor and because of this we will use lasso regularization which would take this useles predictorand multiple x by the beta weights
