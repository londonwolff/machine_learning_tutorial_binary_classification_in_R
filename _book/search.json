[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beginners Guide to Machine Learning Binary Classification Problems in R",
    "section": "",
    "text": "1 Overview & Scope of Tutorial\nThe purpose of this tutorial is to give a brief example of how to use machine learning classification techniques on an example dataset from the animal behavior field. In this tutorial I will cover 1) the five main types of classification algorithms and when to use them; 2) how to set up and clean data to use in classification machine learning, and 3) how to run models and compare the outcomes. I propose three different questions that could be answered with machine learning classification to showcase how the types of predictor and outcome variables you are working with impacts what models to run and with what hyperparameters. (Throughout the tutorial, you’ll see highlighted vocab terms like hyperparameters - click to see the definition and then click that word again to return to the text). The data used in this tutorial is from a team science open access dataset from the Many Dogs Project, and the number and type of variables makes it a good example dataset to use with classification machine learning. For a more extensive description of the dataset please see the data description section. A beginners level of knowledge in using R, Rstudio, and GitHub is recommended.\nThis tutorial is not meant to be an exhaustive list of what can be done with machine learning. I only cover supervised learning for binary classification problems. I do not cover any inductive processes like deep learning (aka neural nets), semi-supervised learning, unsupervised learning, or how to deal with continuous dependent variables. See links within text for more detail on these applications. This tutorial also assumes you have a basic understanding of simple statistical concepts like distributions and categorical vs. continuous variables, and some exposure to basic models like linear regression. If the terms in that last sentence weren’t familiar to you, check out the resources at PsyTeachR for an introduction to statistics.\n\n1.0.1 Attribution and Licensing\nThis guide is licensed under a Creative Commons Attribution - NonCommercial 4.0 International License. Please share and distribute this tutorial to people who would find it useful! This guide should not be used for commercial purposes and you must give appropriate credit and indicate what changes were made if any.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview & Scope of Tutorial</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "whatisML.html",
    "href": "whatisML.html",
    "title": "2  What is Machine Learning?",
    "section": "",
    "text": "2.1 Definitions\nWhile I mentioned in the overview that I am not going to be covering deep learning (aka neural nets), semi-supervised learning, unsupervised learning, or how to deal with quantitative predictions, It’s helpful to feel like you are hip with the lingo if you know the difference between all these terms. The difference between supervised, unsupervised, and semi-supervised is very simple. In supervised learning your data has predictors and an outcome variable. In unsupervised learning your data has predictors, but no responses. Instead of looking at how your dependent variable is impacted by an independent variable, unsupervised machine learning has a number of independent/predictor variables and looks to create groups within data based on similarities and differences. For example, we might be interested in predicting what regions of the brain show similar connectivity patterns, without having any prior knowledge of what regions are functionally related. In semi-supervised your data has a mix of both: some of your data has responses and some doesn’t, and you do a mix of both methods on each subset of the data. Deep learning is a term that means you are using a specific model class called neural networks. Neural networks can be either supervised, semi-supervised, or unsupervised. It just depends on what kind of independent and dependent variables you are feeding the model. For more information on when to use what algorithm see this blog post. There are more models within machine learning than would be possible to speak on and more are being created as I write this. Some of these models can be used in both classification that I discuss here and regression, but I will only be discussing them in terms of classification.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Machine Learning?</span>"
    ]
  },
  {
    "objectID": "whatisclassML.html",
    "href": "whatisclassML.html",
    "title": "3  What is Classification Machine Learning and How Do You Do It?",
    "section": "",
    "text": "So, if that’s what machine learning is: what is classification machine learning? Great question! I’m so glad you asked! In this tutorial we are interested in data sets where the outcome variable (aka dependent variable) is discrete or ordinal, and we are therefore trying to predict what category an observation will end up in. That is called classification. Classification falls under the supervised learning umbrella because there are predictors that are either continuous, categorical, or binary, and we are interested in classifying these discrete categorical outcome variable based on predictors.\nTo conceptualize what you should do on a classification machine learning journey let’s break it down into 5 simple parts:\n\n\n\n1. Explore data and check assumptions\n2. Randomly partition the data into training and test sets\n3. Run repeated cross validation\n4. Choose an algorithm based on variable types and assumptions and run models with hyperparameters\n5. Assess how well each model performs on the testing data set\n\nYOU DID MACHINE LEARNING!\n\nSo simple, right? Okay, so maybe you aren’t ready to go run models on your own yet ;) Let’s discuss what each step does and why before we jump into how to do it:\n\nExplore data and check assumptions: Before you can run any models you need to explore your dataset to understand what types of variables you have, clean up the data, and deal with any missing values. Once these steps are complete you must check if the properties of your dataset meet the necessary criteria for each potential algorithm. Each algorithm has its own set of assumptions that must be met for it to work as intended. If the assumptions of an algorithm are violated in the working dataset than models created by that specific algorithm are often useless*. If your model assumes condition x, but your data violates condition x, then the model outcomes will be less predictive and, in some cases, essentially worthless. When assumptions are not met, the model’s outputs can be biased, inefficient, or inaccurate, leading to poor performance or incorrect conclusions. Once you know which algorithms assumptions are met or violated you can choose the appropriate algorithms to use in step 4.\nRandomly partition the data: Breaking data into training and test sets allows you to reserve the testing data to evaluate how the model you create performs on new, unseen data. The test set is the unseen data, and the training data is what you use to estimate model parameters. TO partition the data into training and test sets, it makes sense to randomly partition them to avoid any weird order effect that may go along with data collection. But a problem with randomly partitioning data is that every time you run the analysis, you’ll use a different random partition, which can lead to a slightly different result. This means that your analysis is not fully reproducible. So anyone running your code (including you, when you comeback to rerun your analyses!) will not get exactly the same results. To ensure reproducible results, you should set a seed (i.e., a fixed starting point for the randomization process). Anyone who uses that seed will use the same randomization process.\nRun repeated cross validation: Above you partitioned the data once, which is the most basic form of cross validation. However, to partitioning the data only once can result in weird, uneven distributions of data between the training and testing sets. Instead, we run a bunch of these partitions (called repeated cross validation) and average over them to reduce bias and variance. If you forgo this step, you will likely get a more biased estimation of which model is going to accurately predict new data because of random noise in the training data, rather than the true relationship between your predictors and outcome variables as they exist in the world outside your sample.\nChoose an algorithm and run with hyperparameters: Using the characteristics of your data (number of predictors, type of outcome variables, etc.) and model assumptions (collinearity, outliers, normality, etc.) you can narrow down your analysis to a number of models that you could use to make predictions with a specific dataset. However, you cannot know what single model will run best until you actually run the models and compare the outcomes. This step also includes adding in any necessary hyperparameters the computer needs to run a specific model.\nAssess model performance: After you have run all the models that you are interested in, you then need to determine how well each model predicted the test set or unseen data. Generalization error is the term for this. It is a measure of how well a models predictions match the true values in the training data. In things like a logistic regression this can be as simple as calculating the percent of categories predicted correctly, or as complex as creating an ROC graph. Basically, in this step we are using standardized metrics to evaluate how well each possible model predicted the testing data.\n\n\n* You will notice the words algorithm and model used throughout this document. They refer to distinct pieces of information and are not interchangeable. When the word algorithm is used it refers to a framework/class of machine learning algorithm that could possibly be used to create a specific model. A model refers to the actual model complete with all structures and hyperparameters. For examples an algorithm type is linear regression but a model is a a linear regression run with outcome x, predictors, a b and c and a 10-fold cross validation procedure.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>What is Classification Machine Learning and How Do You Do It?</span>"
    ]
  },
  {
    "objectID": "datadescription.html",
    "href": "datadescription.html",
    "title": "4  Data Description",
    "section": "",
    "text": "The data being used in this tutorial is from the ManyDogs Project, a large multi-lab collaboration where dogs from across the world all completed the same two behavioral tasks and information about the dog was collected via survey from owners. The two behavioral tasks were variations on a two-alternative object choice task. Simply put, dogs were asked to choose between two cups. In both tasks an unknown human pointed at the “correct” cup that had a treat under it. In one task, ostensive, pointing was accompanied with eye gazing and calling the dog’s name. In the other condition, non-ostensive, the unknown human simply pointed without other cues. There were also warm-up trials to familiarize the dog with the task as well as an odor control condition. I will not be using the odor control condition or warm up tasks in this tutorial but they are available in the dataset to explore later on as possible predictors of performance. The survey data collected included breed, age, sex of the dog, training frequency, and values from the CBARQ. The CBARQ is a widely used tool in the field of canine behavioral research, which assesses aspects of dog behavior, temperament, and personality. For a complete list of all variables in the dataset see the ManyDogs GitHub repository README file.\n\n\n\n\n\n\n\n\n\nThis dataset is a great example to use when investigating machine learning predictive classification models, as it has many possible predictors to investigate with a discrete binary dependent variable (i.e. whether the dog chose correctly). Furthermore, all the data from this project is available to anyone to share or adapt with attribution, which makes it ideal to use as a learning tool. To work through this tutorial with me, you will first need to download the data from the GitHub repository associated with the project and/or create a local clone of the project repo on your computer. I recommend using the GitHub desktop application to easily get and give information from/to a GitHub repository using a point and click method instead of the command line.",
    "crumbs": [
      "Data & Research Questions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Description</span>"
    ]
  },
  {
    "objectID": "researchqs.html",
    "href": "researchqs.html",
    "title": "6  Research Questions to Investigate",
    "section": "",
    "text": "In this tutorial we will be assessing how to answer the following three questions:\n\nDoes the amount of training a dog has completed (as reported by their owner on a survey) predict whether a dog will perform below or above chance (i.e., 50%) on the ostensive behavioral task?\nDoes the amount of training a dog has completed predict whether a dog will perform below or above chance on the nonostensive behavioral task?\nHow much of the variability in whether a dog performs better on non-ostensive tasks compared to ostensive tasks can be explained by its behavioral traits (training, sex, age, etc.)?\n\nThe first two questions have a very similar structure to each other, with a slight change in our outcome (i.e., our dependent variable). These are the simpler forms of classification models, which compare one continuous variable to one discrete binary variable. Question three adds a number of predictor variables, yet the binary discrete dependent variable is changed to add information about both types of behavioral tasks instead of looking at one at a time. Hopefully by working through these three questions you will get a better sense of how to choose what models to run, how to choose between models to get the best performing model, and how to change code in each model to account for different parameters.",
    "crumbs": [
      "Data & Research Questions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Research Questions to Investigate</span>"
    ]
  },
  {
    "objectID": "whatisML.html#definitions-and-vocab",
    "href": "whatisML.html#definitions-and-vocab",
    "title": "2  What is Machine Learning?",
    "section": "",
    "text": "ml_cheet_sheet",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Machine Learning?</span>"
    ]
  },
  {
    "objectID": "beforeyoubegin.html",
    "href": "beforeyoubegin.html",
    "title": "5  Set Up your R Document",
    "section": "",
    "text": "Make sure you have the latest versions of R and Rstudio downloaded and open. You will then need to create a new project in R, name it, and then open an R document, either a script or a markdown file to begin following along with the tutorial.\nThe following libraries (often called dependencies) are needed to run this tutorial in R. Before you code anything, make sure you have the following packages installed and updated. The below code should be copied and pasted to your R document before any other code is written as you need the tools in these libraries/dependencies to complete the tutorial. Throughout the code chunks in this tutorial I have written comments that tell you what each step in the data process does.\n\nlibrary(tidyverse) # for data wrangling\nlibrary(knitr) # for showing you pretty pictures and making tables\nlibrary(here) # for keeping track of where to get files needed for this tutorial\nlibrary(janitor) # for examining and cleaning data\nlibrary(ggpubr) # to make Q-Q plots for testing normality\nlibrary(car) # for making component plus resistance (CR) plots to check for linearity\nlibrary(mlr) # for doing machine learning in R!\nlibrary(parallelMap) # for running code using all of the available processing power on your computer\nlibrary(parallel) # for running code using all of the available processing power on your computer\nlibrary(randomForest) # for running the random forest algorithm\n\n\n5.0.1 Open and View Data\nOnce you have downloaded the data files from the GitHub repo and loaded all the necessary dependencies make sure the data file manydogs_etal_2024_data is in the same file path as the R file you are working in. When they are in the same file path you can use the following code to read the data into your R file and begin working with the data!\n\nmanydogs_data &lt;- read.csv(\"manydogs_etal_2024_data.csv\") # Read in the data file I will be using\n\nhead(colnames(manydogs_data), n = 20) #show first 20 names of each column in the data\n\n [1] \"date\"                     \"site\"                    \n [3] \"subject_id\"               \"experiment_status\"       \n [5] \"owned_status\"             \"birthdate\"               \n [7] \"sex\"                      \"age\"                     \n [9] \"desexed\"                  \"purebred\"                \n[11] \"breed\"                    \"breed_registry\"          \n[13] \"mixed_breed\"              \"communication_method\"    \n[15] \"gesture_frequency\"        \"gaze_follow\"             \n[17] \"training_type\"            \"training_freq_puppy\"     \n[19] \"training_freq_neighbor\"   \"training_freq_obedience1\"",
    "crumbs": [
      "Data & Research Questions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Set Up your R Document</span>"
    ]
  },
  {
    "objectID": "whatisML.html#definitions",
    "href": "whatisML.html#definitions",
    "title": "2  What is Machine Learning?",
    "section": "",
    "text": "ml_cheet_sheet",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Machine Learning?</span>"
    ]
  },
  {
    "objectID": "whatisML.html#the-case-for-machine-learning",
    "href": "whatisML.html#the-case-for-machine-learning",
    "title": "2  What is Machine Learning?",
    "section": "2.2 The Case for Machine Learning",
    "text": "2.2 The Case for Machine Learning\nIf you already use standard modeling practices why should you learn machine learning? That question should be answered by a large group of statisticians arguing at a conference, but in the mean time, I am going to give a few brief arguments for Machine learning to be more widely used in social sciences. Machine learning is worth the time to learn because it can help alleviate some of the systemic replication issues in psychology and other fields as it is a more analytic process of making claims with data. It also allows for prediction instead of just inference. Current modeling practices often have some form of subjective decision making. We choose a theory a priori, we run some tests that are designed to support our theory (in principle we run tests that challenge our theory but in reality we often set ourselves up to do the opposite, if we are doing null hypothesis testing), and there are countless decisions we make in the meantime to shape the data into results consistent with our theory. Machine learning bypasses some of this by letting the analysis follow the data, using the characteristics of the data to make some of those decisions rather than human subjectivity.\n\n* Even in iterative analyses, where information from one step is used to inform the analytic decision at the next step you still need to specify the terms of each step of that iterative process before you run it. Unlike Machine learning, which is adaptive and can automatically adjust and learn based on the data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>What is Machine Learning?</span>"
    ]
  },
  {
    "objectID": "datacleaning.html",
    "href": "datacleaning.html",
    "title": "7  Data Set-up: Tidying and Feature Selection",
    "section": "",
    "text": "Before you can begin working with your data you must make sure that each row is a single observation, and each column is a single variable/predictor. This type of data set-up or “wrangling” is known as “tidy data”. There are many readily available tutorials and textbooks that help you understand tidy data and how to clean and wranglde data to make it tidy. I recommend the Tidyverse chapter in the R data science textbook to start. Thankfully, the dataset from ManyDogs is already tidy so for this tutorial we can skip this step.\nAfter your data is tidy, the next step before is to complete feature selection. Feature selection is a fancy term for removing variables you aren’t going to analyze and creating new ones by computing any necessary variables.\nMany of the changes you complete in this step are decided on through domain knowledge: applying what you know about the field of research to make judgement calls on what is and isn’t important to the model/data. The rest of our decisions depend on our specific research hypotheses. Anything in the data set that does not specifically pertain to our research hypotheses need to be eliminated to increase statistical power and to compute the model faster to save computing resources. Commonly deleted variables at this stage might include meta-data such as time of day when a survey was completed, or individual scale items when we have calculated the total scores. When domain knowledge doesn’t suffice because you are working in a relatively new field, or past studies have conflicting information, you will want to let machine learning algorithms help choose what to keep and eliminate. See the regularization section for more information.\nIn our ManyDogs example, our feature selection will include transforming all the levels of scales from words to values, add 0s to denote no household dogs other than the one in the study, computing average scores for each section of the CBARQ, and computing our binary outcome variables. We will then cut any remaining columns that will not be needed for analysis.\nReminder: The following code should be copied and pasted to get you to the needed dataset that we will be using in this tutorial.\n\n#The following code creates a new dataframe where all of the scales are changed from words to numbers so they can be used as discrete categorical data in the models we will create in this tutorial\nmanydogs_data_transformed &lt;- manydogs_data |&gt;\n  mutate(across(c(cbarq_train_1:cbarq_train_8), ~case_when(   #Change all scales from words to numbers\n    . == \"Never\" ~0,\n    . == \"Seldom\" ~1,\n    . == \"Sometimes\" ~2,\n    . == \"Usually\" ~3,\n    . == \"Always\" ~4\n  ))) |&gt; \n  mutate(across(c(cbarq_aggression_1:cbarq_aggression_27), ~case_when(\n    . == \"No aggression\" ~ 0, \n    . == \"Mild aggression\" ~ 1,\n    . == \"Moderate aggression\" ~2,\n    . == \"High aggression\" ~3, \n    . == \"Serious aggression\" ~4\n  ))) |&gt; \n  mutate(across(c(cbarq_fear_1:cbarq_fear_18), ~case_when(\n    . == \"No fear\" ~ 0, \n    . == \"Mild fear\" ~ 1,\n    . == \"Moderate fear\" ~2,\n    . == \"High fear\" ~3, \n    . == \"Extreme fear\" ~4\n  ))) |&gt; \n  mutate(across(c(cbarq_separation_1:cbarq_separation_8), ~case_when(\n    . == \"Never\" ~0,\n    . == \"Seldom\" ~1,\n    . == \"Sometimes\" ~2,\n    . == \"Usually\" ~3,\n    . == \"Always\" ~4\n  ))) |&gt; \n  mutate(across(c(cbarq_excitability_1:cbarq_excitability_6), ~case_when(\n    . == \"No excitability\" ~ 0, \n    . == \"Mild excitability\" ~ 1,\n    . == \"Moderate excitability\" ~2,\n    . == \"High excitability\" ~3, \n    . == \"Extreme excitability\" ~4\n  ))) |&gt; \n  mutate(across(c(cbarq_attachment_1:cbarq_attachment_6), ~case_when(\n    . == \"Never\" ~0,\n    . == \"Seldom\" ~1,\n    . == \"Sometimes\" ~2,\n    . == \"Usually\" ~3,\n    . == \"Always\" ~4\n  ))) |&gt; \n  mutate(across(c(cbarq_miscellaneous_1:cbarq_miscellaneous_27), ~case_when(\n    . == \"Never\" ~0,\n    . == \"Seldom\" ~1,\n    . == \"Sometimes\" ~2,\n    . == \"Usually\" ~3,\n    . == \"Always\" ~4\n  ))) |&gt; \n  mutate(sex = case_when(sex == \"Female\" ~1, sex == \"Male\" ~2),\n         desexed = case_when(desexed == \"Yes\" ~1, desexed == \"No\" ~2),\n         purebred = case_when(purebred == \"Yes\"~1, purebred == \"No\" ~2),\n         gaze_follow = case_when(gaze_follow == \"Never\" ~ 1, \n                                 gaze_follow == \"Seldom\" ~2,\n                                 gaze_follow == \"Sometimes\"~3,\n                                 gaze_follow == \"Usually\" ~4,\n                                 gaze_follow == \"Always\" ~5),\n         num_household_dogs = ifelse(is.na(num_household_dogs), 0, num_household_dogs)) |&gt;  #add 0s to indicate when no other dogs were in the household\n  mutate(across(c(cbarq_train_1:cbarq_train_8,cbarq_aggression_1:cbarq_miscellaneous_27), as.numeric))  #change character columns to numeric columns so R understand how to use them\n\n\n#Create a data frame that calculates composite scores for each subset of the scale and computes our three binary outcome variables\nmanydogs_feature_selection &lt;- manydogs_data_transformed |&gt; \n  mutate(training_score = rowMeans(select(manydogs_data_transformed,   #compute average scores of training\n                             starts_with(\"cbarq_train_\")), na.rm = TRUE),\n         aggression_score= rowMeans(select(manydogs_data_transformed,   #compute average scores of aggression\n                             starts_with(\"cbarq_aggression_\")), na.rm = TRUE),\n         fear_score= rowMeans(select(manydogs_data_transformed,    #compute average scores of fear\n                         starts_with(\"cbarq_fear_\")), na.rm = TRUE),\n         separation_score= rowMeans(select(manydogs_data_transformed,    ##compute average scores of separation issues\n                              starts_with(\"cbarq_separation_\")), na.rm = TRUE),\n         excitability_score = rowMeans(select(manydogs_data_transformed,    #compute average scores of excitability\n                                 starts_with(\"cbarq_excitability_\")), na.rm = TRUE),\n         attachment_score= rowMeans(select(manydogs_data_transformed,     #compute average scores of attachment\n                              starts_with(\"cbarq_attachment_\")), na.rm = TRUE),\n         miscellaneous_score= rowMeans(select(manydogs_data_transformed,    #compute average scores of miscellaneous behavior issues\n                                 starts_with(\"cbarq_miscellaneous_\")), na.rm = TRUE),\n         ostensive = rowMeans(select(manydogs_data_transformed, \n                                     starts_with(\"ostensive_\")), na.rm = TRUE), #create proportion correct on ostensive task\n         ostensive_binary = ifelse(ostensive &lt;= 0.5, 0, 1), #create column that notes if a dog performed over or under chance at the ostensive task\n         nonostensive = rowMeans(select(manydogs_data_transformed,\n                                        starts_with(\"nonostensive_\")), na.rm = TRUE), #create proportion correct on nonostensive task\n         nonostensive_binary = ifelse(nonostensive &lt;= 0.5, 0, 1),   #create column that notes if a dog performed over or under chance at the nonostensive task\n         os_best = ifelse(nonostensive &gt; ostensive, 1, 0)) |&gt;   #create column that notes if the dog was better at the nonostensive task\n  select(c(sex,age,gaze_follow,num_household_dogs:environment,training_score:os_best))#grab the columns we will be using for the analysis in this tutorial",
    "crumbs": [
      "Step 1: Exploring Data & Checking Assumptions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Set-up: Tidying and Feature Selection</span>"
    ]
  },
  {
    "objectID": "twoassumptions.html",
    "href": "twoassumptions.html",
    "title": "8  Two Model Assumptions to Rule Them All: Handling N>P and Missing Data",
    "section": "",
    "text": "While all models rely on specific assumptions to find patterns in data, there are two key assumptions that are generally applicable across most models. The first assumption is that the number of observations is greater than the number of predictors. The second assumption is that there is no missing data. Unfortunately, in practicality missing data is almost always present. Therefore, when you have missing data it must be transformed either by removal or imputation (a fancy word for filling in the missing values with a placeholder value like mean, median, or mode). Let’s figure out how to deal with them.\n\n8.0.1 The N&gt;P Problem AKA Dimensionality?\nOne underlying assumption across all of machine learning is that you have more observations (n) than you have predictors (p), n&gt;p. This problem is also sometimes refereed to as dimensionality. High dimensionality is the same thing as making sure you don’t have n &lt; p. The number of observations must be large than the number of predictors because analytic models do a bad job of parsing out what predictors are important if they don’t have multiple observations per predicting factor. If you have fewer observations than predictors (n&lt;p), your models may still run without error, but the outputs given will often not be meaningful or predictive. These models overfit the training data and are poor at predicting new data.\nFor most problems in psychology with small datasets (n &lt; 1000), you need to make sure that you have more observations (n) than you have predictors. For instance, in the dataset we are working with today, there are 704 observations and 18 predictors. We know this because our data is tidy so each row is an observation and each column is a predictor, with tidy data all I need to look at to see if n&gt;p is true is whether there are more rows than columns. 704 is larger than 18 (phew!) so we do not need to worry about decreasing the number of predictors to make it satisfy this condition.\nHowever, even some cases where there are more predictors than observations (n &lt; p) can be managed with fancier algorithms and hyperparameters that need more time and computing power - but that is outside the scope of this tutorial. In psychology, researchers using neuroimaging often have to deal with n &lt; p, as the number of voxels or regions of the brain being measured is much more than the number of subjects for which they can collect imaging data. One way of dealing with this is to do feature selection via regularization to reduce the number of possible predictors. There are a few ways to deal with datasets with more predictors than observations.\n\n\n8.0.2 Missing Data\nUnfortunately, missing data is a huge issue for machine learning algorithms. Having a high proportion of missing data (anything greater than 10% in any column) can be detrimental to the predictive accuracy of any model, regardless of whether all other assumptions are met. And some models simply cannot run with any missing data.\nFortunately, there are a number of ways that you can deal with missing data. The simplest (but least desirable) way is to simply not use any observations with missing data. This is the least desired solution because the less data you have, the worse algorithms perform at prediction. Taking out an entire observation because of one missing data point limits the amount of data you have, causing the algorithm to perform worse. Another way of dealing with missing data is to replace missing values by imputing them. Imputing is a fancy word for replacing missing values with other values. One way to replace missing values would be to add 0s to everything that was NA. However, this option can be problematic because it could lead to a model with a ton of bias, or a model showing relationships that are not present because of how a lot of zeros skew the data distributions. Another option of imputing data is to replace the NAs with a value that represents a measure of central tendency. With continuous data, you can replace missing values with the mean or median of the column. For normally distributed data, the mean is most representative and can be used. For data with strong outliers and/or a skewed distribution, the median offers a good option. With categorical data, you can replace missing values with the most frequent number found in the column. Another option (which is outside the scope of this tutorial) is fitting a regression or K nearest neighbors model to the data, and filling in the missing values with the value that these models predict would go in the missing slot. There are good resources for using a KNN model for imputation and a regression model for imputation. The most appropriate imputation method needs to be based on the data set. We are now going to walk through several examples to figure out how to find the best way that fits the data.\nTo check missing data and transform data accordingly, the first step is to visualize how much missing data you have and where it occurs in your data.\n\n#create vector with how many NAs are in each column in the dataset\nmissing_frequency_in_manydogs_data &lt;- colSums(is.na(manydogs_feature_selection))  \n\n#Create a barplot for the values frequency of missing values in the manydogs dataset\nmissing_values_barplot &lt;- barplot(missing_frequency_in_manydogs_data,\n                              main = \"Number of Missing Values per Column\",\n                              ylab = \"Number of Missing Values\",\n                              las = 2,\n                              cex.names = .6)\n\n\n\n\n\n\n\n\nYou can see in the above table that we have missing values that need to be dealt with in almost all of our columns. The next step in the missing data process is to summarize our columns. We summarize columns for three reasons. The first is to see what percentage of the column is missing to help us decide when we should remove observations versus when to impute missing values. The second reason is to see the central tendencies of each column. This works in tandem with our third reason, namely, to understand the skew of our columns and begin to do exploratory data analysis.\n\n8.0.2.1 Imputing Continuous Missing Data\nTo calculate and view the mean, median, min, max, and number of NAs per continuous column, we can use the summary() function. We have 10 columns that are continuous and so we will use the select() function to grab just those 10 columns.\n\nmanydogs_summary &lt;- manydogs_feature_selection |&gt;  #summarize continuous data\n  select(c(age,training_score:ostensive,nonostensive)) |&gt;  #grab the 8 columns we needed\n  summary()  #Show me a summary of all the columns that I selected\n\nmanydogs_summary\n\n      age         training_score  aggression_score    fear_score    \n Min.   : 0.300   Min.   :0.875   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.: 1.800   1st Qu.:2.250   1st Qu.:0.09416   1st Qu.:0.2778  \n Median : 3.900   Median :2.500   Median :0.30769   Median :0.5882  \n Mean   : 4.435   Mean   :2.451   Mean   :0.40090   Mean   :0.6763  \n 3rd Qu.: 6.400   3rd Qu.:2.714   3rd Qu.:0.58042   3rd Qu.:0.9444  \n Max.   :20.800   Max.   :3.750   Max.   :2.03846   Max.   :2.4444  \n NA's   :26       NA's   :7       NA's   :200       NA's   :203     \n separation_score excitability_score attachment_score miscellaneous_score\n Min.   :0.0000   Min.   :0.000      Min.   :0.000    Min.   :0.0000     \n 1st Qu.:0.0000   1st Qu.:1.500      1st Qu.:1.500    1st Qu.:0.6296     \n Median :0.2500   Median :2.000      Median :2.000    Median :0.8750     \n Mean   :0.4784   Mean   :2.007      Mean   :2.012    Mean   :0.9040     \n 3rd Qu.:0.7500   3rd Qu.:2.500      3rd Qu.:2.500    3rd Qu.:1.1482     \n Max.   :3.5000   Max.   :4.000      Max.   :4.000    Max.   :3.0000     \n NA's   :207      NA's   :204        NA's   :205      NA's   :204        \n   ostensive       nonostensive  \n Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.4444   1st Qu.:0.375  \n Median :0.5000   Median :0.500  \n Mean   :0.5213   Mean   :0.504  \n 3rd Qu.:0.6250   3rd Qu.:0.625  \n Max.   :1.0000   Max.   :1.000  \n NA's   :135      NA's   :135    \n\n\nNow that we have the number of missing values per column, we will find the percentage of missing values per column to see if any of the columns are more than 10% missing values.\n\n# Grab just the continuous predictors\nmanydogs_missing &lt;- manydogs_feature_selection |&gt; \n  select(c(age,training_score:ostensive,nonostensive))\n\n# Calculate the percentage of missing values for each column\nmissing_percent &lt;- colMeans(is.na(manydogs_missing)) * 100\n\n# Print the missing summary\nmissing_percent\n\n                age      training_score    aggression_score          fear_score \n          3.6931818           0.9943182          28.4090909          28.8352273 \n   separation_score  excitability_score    attachment_score miscellaneous_score \n         29.4034091          28.9772727          29.1193182          28.9772727 \n          ostensive        nonostensive \n         19.1761364          19.1761364 \n\n\nAll but the first two columns (age and training score) had a missing value percentage of more than 10%, so it is best NOT to simply delete the missing values as that would be a large portion of our data and would negatively impact model performance. If none of the variables (columns) have more than 10% missing data, we could get away with pairwise deletion for those few missing values. Instead, we will impute the data, and in this case we use central tendency values to replace NAs. To help us decide if we should replace continuous missing values with the column mean or median, we need to look at a histogram to investigate if there is strong skew or outliers that could pull the mean too far in one direction. We will do this using ggplot()\n\n# Subset the dataframe to include only the specified columns\nsubset_data &lt;- manydogs_feature_selection |&gt; \n  select(\"age\", \"training_score\", \"aggression_score\", \"fear_score\", \n                     \"separation_score\", \"excitability_score\", \"attachment_score\", \n                     \"miscellaneous_score\", \"ostensive\", \"nonostensive\")\n\n# Convert the data to long format, specifying the variable column\nsubset_data_long &lt;- pivot_longer(subset_data, cols = everything(), names_to = \"variable\", values_to = \"value\", names_transform = list(variable = as.character))\n\n# Create histograms using facet_wrap\nggplot(subset_data_long, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ variable, scales = \"free_x\")\n\n\n\n\n\n\n\n\nPlots for training_score, attachment_score, excitability_score, miscellaneous_score, nonostensive, and ostensive are normally distributed with few strong outliers so we will replace missing values with the mean of each column. All other plots are skewed and/or have strong outliers, so we will replace these missing values with the median of the column.\n\n#create a data frame that replaces missing data with the column mean or median calculated above in summary tables\nmanydogs_missing_cont &lt;- manydogs_feature_selection |&gt; \n  mutate(age=replace_na(age, median(age, na.rm=TRUE)), #change NAs in the age column to the median of the age column\n        training_score=replace_na(training_score, mean(training_score, na.rm=TRUE)), #change NAs in the training column to the mean of the column\n        aggression_score=replace_na(aggression_score, median(aggression_score, na.rm=TRUE)),#change NAs in aggression column to median of the column\n        fear_score=replace_na(fear_score, median(fear_score, na.rm=TRUE)), #change NAs in fear column to median of the column\n        separation_score=replace_na(separation_score, median(separation_score, na.rm=TRUE)), #change NAs in separation column to the median of the column\n        excitability_score=replace_na(excitability_score, mean(excitability_score, na.rm=TRUE)), #change NAs in excitability column to mean of the column\n        attachment_score=replace_na(attachment_score, mean(attachment_score, na.rm=TRUE)),#change NAs in attachment column to mean of the column\n        miscellaneous_score=replace_na(miscellaneous_score, mean(miscellaneous_score, na.rm=TRUE)), #change NAs in miscellaneous column to mean of the column\n        ostensive=replace_na(ostensive, mean(ostensive, na.rm = TRUE)),\n        nonostensive=replace_na(nonostensive, mean(nonostensive, na.rm = TRUE)))\n\n\n\n8.0.2.2 Imputing categorical missing data\nNow we have to summarize the categorical columns to assess what frequencies to add in place of the missing data. To get the mode of each column we will make our own function called get_mode(). We will then apply this function within summarise(). We have 7 columns that are categorical, so we need to find the mode for each of these columns.\n\n#create a function that finds the mode of a column\nget_mode &lt;- function(x) {\n  uniq_x &lt;- unique(x)\n  uniq_x[which.max(tabulate(match(x, uniq_x)))]\n}\n\n#Selects the columns we are interested in and finds the mode\nmanydogs_feature_selection |&gt; \n  select(sex, desexed, purebred, gaze_follow, ostensive_binary, nonostensive_binary, os_best) |&gt; \n  summarise(across(everything(), get_mode))\n\n  sex desexed purebred gaze_follow ostensive_binary nonostensive_binary os_best\n1   1       1        1           3                0                   0       0\n\n\nNow we will replace NAs with the category with the highest frequency in each column (the mode).\n\n# add highest frequency per column to missing data in the data frame\nmanydogs_missing_handled &lt;- manydogs_missing_cont |&gt;\n  mutate(sex = replace_na(sex, 1),  #change the NAs in the sex column to 1\n         desexed = replace_na(desexed, 1),\n         purebred = replace_na(purebred, 1),\n         gaze_follow = replace_na(gaze_follow, 3),\n         ostensive_binary = replace_na(ostensive_binary, 0),\n         nonostensive_binary = replace_na(nonostensive_binary, 0),\n         os_best = replace_na(os_best, 0))\n\nNow if you check how many missing values are in each column you will see that it is 0 for each column!\n\nmissing_frequency_after_missing_handled &lt;- colSums(is.na(manydogs_missing_handled)) #Add together any time that a column has an NA\n\nmissing_frequency_after_missing_handled\n\n                  X                 sex                 age             desexed \n                  0                   0                   0                   0 \n           purebred         gaze_follow      training_score    aggression_score \n                  0                   0                   0                   0 \n         fear_score    separation_score  excitability_score    attachment_score \n                  0                   0                   0                   0 \nmiscellaneous_score           ostensive    ostensive_binary        nonostensive \n                  0                   0                   0                   0 \nnonostensive_binary             os_best \n                  0                   0",
    "crumbs": [
      "Step 1: Exploring Data & Checking Assumptions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Two Model Assumptions to Rule Them All: Handling N\\>P and Missing Data</span>"
    ]
  },
  {
    "objectID": "regularization.html",
    "href": "regularization.html",
    "title": "11  Regularization AKA how to fix overfitting and collinearity",
    "section": "",
    "text": "Now, if you were to consult other machine learning texts, you will notice one word that often comes up… regularization. Regularization only applies to linear models because it is predicated on changing the slope of a fitted line to reduce overfitting issues. For our purposes, we will not be able to use regularization in this tutorial. This is because linear regression is not a classification based model, the assumptions weren’t met for logistic regression, and tuning regularization with support vector machines is outside the scope of this tutorial. If you have no interest in regularization you can skip this section. I’ve included this section in the tutorial because it’s a key concept in machine learning, commonly applied whenever linear or logistic regression is used.\nRegularization helps us to solve two problems that plague statistics: collinearity and overfitting. Often after you collect your data, you will find that some of your predictors are colinear, meaning that two or more of your predictor variables are highly correlated with each other. Overfitting means that your model too closely maps onto your training data and isn’t flexible enough to do well or make accurate predictions for new unseen data. All data represents two sources of variation: the true relationships, and the error in your sample (i.e., what is true of your sample but is not true of the population). The smaller your sample size, the more likely it is that you have captured noise rather than the true relationship between variables. The graphic below helps illustrate how overfitting creates a model reliant on error rather than the true relationship.\n\n\n\noverfitting\n\n\nYou can solve both of these issues at once with something called regularization. Regularization simply adds a penalty term, lambda, to the slope of the fitted line. This additional value added to the slope adds in a little bit of error to the model. This user-added error is a necessary evil to combat some of the noise that occur in models when the data used to construct the model is not totally representative of the underlying truth of the relationship between variables. The choice to regularize is totally up to you, and is based on how much risk tolerance or uncertainty you want to include in your model outcomes. If you have millions of data points from a highly representative sample, adding in that extra error will probably get you further from the truth of the relationship. If you have, like we do in this example, less than 1000 subjects and are trying to generalize to the species level you probably should do regularization as the sample is small and not totally representative of your total population.\nThere are three types of regularization that are commonly used:\n\nLasso Regularization - best to use if you know some of the predictors in your model are useless\nRidge Regularization - best to use if you know all of the predictors in your model are useful\nElastic Net Regularization - best to use if you don’t know how useful/useless your predictors are because it has the best of both worlds (Chill it out take it slow than you’ll rock out the show) - it does the math from lasso + math from ridge to get a new number that is called elastic net.\n\nIn each case, our algorithm creates a value that adds together the loss/cost function, a lambda value, and then multiples this number by a transformation of the beta weights. How the beta weights are transformed is where the three types differ. If you got a little lost in that last sentence, don’t despair; we will describe each of the three parts that contribute to regularization in detail. I’ll clarify this concept using an example you are likely familiar with, linear regression.\nIn a linear regression model, our loss/cost function is the sum of squared residuals. The sum of squared residuals (calculated by adding together all the squared values of the differences between each data point and the estimate of where the line says the data point should be) is a value that represents how well the line did at predicting actual values.\nThe second term, Lambda, can be any positive number. The larger the lambda, the larger the penalty or error you are adding to the model. The lambda is calculated by iteratively going thru various values of lambda until you find one that minimizes the regularization value.\nThe difference between the three types of regularization occur in the third term, which represents how they transform the beta weights of each variable. In Lasso regularization, you multiply by the absolute value of the beta weights (i.e., |slope|). In Ridge regularization you multiply by the squared beta weights (i.e., slope2). In elastic net, you multiply by adding together both penalties (i.e., |slope| + slope2).\nYou use Ridge regularization when you believe all your variables are useful, and Lasso when you suspect some are not, due to the difference between squaring values in Ridge and using absolute values in Lasso. Essentially, it is possible to set a predictor to exactly 0 when you take the absolute value, but when you square a term it can only ever get very small but never exactly 0. Because of this, Lasso and Elastic Net regularization can set some predictor beta weights to exactly 0, eliminating those predictors from the model. On the other hand, Ridge will always have a beta weight value for each predictor. The smaller the beta weight for Ridge regularization the less predictive an individual predictor will be to the model. In this way Lasso and Elastic net regularization allows you to have math take out useless predictors instead of doing it in some semi-arbitrary way (NICE!).\nTwo really important limitations of regularization are:\n\nWhen working with collinear variables, the way the math works behind the scenes, Lasso and Elastic Net will always set the first collinear variable to 0 and keep the second variable, so be mindful of this when you order the model variables. In practical terms, this means that you should enter terms into the model in order of importance based on domain knowledge.\nThese regularization techniques only works with models where the relationship between predictors and the outcome is linear, or the model parameters are linear. Of the methods mentioned in this tutorial, the only algorithms that regularization works are linear regression, logistic regression, and support vector machines.\n\nFor our purposes, we will not be able to use regularization in this tutorial. This is because linear regression is not a classification based model, the assumptions weren’t met for logistic regression, and tuning regularization with support vector machines is outside the scope of this tutorial. However, if we were to run regularization for these questions, we would choose ridge regularization for research questions 1 and 2, as there is only one predictor variable (meaning it would need to be predictive for there to be a model!). For research question 3, where we are interested in which predictor variables out of many are important, we would use Elastic Net regularization because we want the algorithm to help us decide which predictors to use in the model.\ncolinearity: bascially reducse the btea to 0 removing it from the model thus eliminating the possitivbility or colinearity to occur but it did that without having to choose it did it with iteratize learning.\nfor example, we have a model with two useful predictors nad one useless predictor and because of this we will use lasso regularization which would take this useles predictorand multiple x by the beta weights",
    "crumbs": [
      "Step 1: Exploring Data & Checking Assumptions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regularization AKA how to fix overfitting and collinearity</span>"
    ]
  },
  {
    "objectID": "modelassumptions.html",
    "href": "modelassumptions.html",
    "title": "10  Algorithms to Run Based on Assumption Checks",
    "section": "",
    "text": "Now that we know what assumptions were violated and which were true of our data, we need to look at which models will perform well on each of the three research questions. Let’s review the table we made above for each of the assumption checks we did. A “No” means that there was no issue with this model assumption (the assumption was satisfied), while a “Yes” means that there was an issue with the model assumption (the assumption was violated). Take a moment to refresh on each section: where were there problems with each assumption?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nIndependence\nNormality\nNo_Outliers\nLinearity\nHigh_Dimensionality\nFeature_Scaling\n\n\n\n\nResearch Question 1\nYes\nNo\nNo\nYes\nNo\nNo\n\n\nResearch Question 2\nYes\nNo\nNo\nYes\nNo\nNo\n\n\nResearch Question 3\nYes\nYes\nNo\nNo\nNo\nNo\n\n\n\n\n\nAs you can see, we had issues with independence across all three of our research questions. Because our observations are grouped and dependent on each other, we should eliminate the two models where this assumption must be met: Naive Bayes and Logistic Regression. We also had issues with normality in the data to be used for our third research question. This would eliminate Naive Bayes from the possible models for the third question, but since we already eliminated this model, its violation of that assumption does not change our decision. There were no issues with feature scaling, high dimensionality, linearity, or strong outliers.\nTo hear more about choosing a model see step 4A below.\nNow that I told you all about assumption checks and how important they are, I’ll let you in on a dirty little secret: if all you care about is predicting what category a subject belongs to and nothing about interpretability (and you are running multiple models), it doesn’t really matter if the assumptions are met. 🤯 This is because models whose assumptions are not met will perform worse than models that do meet assumptions, though in some cases, the violations may be minor enough to not significantly affect prediction accuracy. However, it is never a good idea to run algorithms without understanding your data and the relationships within your data. In research especially, we are rarely concerned only with prediction. If you want to understand the real-world phenomena the model is approximating and/or why a particular subject is in a specific category, you need interpretability. Furthermore, running models is time consuming and computationally expensive, and it is best practice to go through all the pre-processing steps before running a model to save resources. The only time you can run models without checking assumptions is when you want an input/output machine that will tell you what category something belongs based solely on features.",
    "crumbs": [
      "Step 1: Exploring Data & Checking Assumptions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Algorithms to Run Based on Assumption Checks</span>"
    ]
  },
  {
    "objectID": "otherassumptions.html",
    "href": "otherassumptions.html",
    "title": "9  Other Model Assumptions",
    "section": "",
    "text": "The first step to understanding whether a machine learning model will be interpretable is understanding what underlying assumptions that model violates or complies with. Assumptions are concepts or conditions that a statistical model relies on for effectiveness. Each model relies on different assumptions, and knowing what model assumes what conditions is a key factor in deciding what model to use with your datasets. This is because if your model assumes condition x, but your data violates condition x, then the model outcomes will be less predictive and, in some cases, essentially useless. When assumptions are not met, the model’s outputs can be biased, inefficient, or inaccurate, leading to poor performance or incorrect conclusions. Every model has different assumptions that must be checked. For example, a logistic regression model assumes noncolinearity, independence of observations, and no strong outliers, while a decision tree model doesn’t assume independence and gives no assumption of linearity. It is best to look at each model independently when deciding what assumptions should be checked, and which are irrelevant.\nIn classification machine learning, most algorithms fall into one of five categories: Logistic regression, K nearest neighbors (KNN), Naive Bayes, Decision trees/Random forest, or Support Vector Machines (SVM). One of the benefits of using machine learning algorithms is you have a wider range of models that can deal with data that are interdependent and not normally distributed than typical statistical models like linear or logistic regression. As you can see in the below table, decision trees/Random forest do not have any underlying assumptions that need to be pre checked before running. Support Vector Machines only assume that you have scaled your predictor. K nearest neighbors (KNN) assumes feature scaling and that the data doesn’t suffer from high-dimensionality, which is another way of saying as observations approach the number of predictors. Naive Bayes has the second most constraints on our list, as it assumes both that your predictor variables are independent of each other (which is usually violated) and that any continuous predictor variables are normally distributed. Logistic regression has more limitations than our other algorithms, as it assumes that predictor variables are independent of each other, that there aren’t strong outliers, that there is a linear relationship between each predictor and the log odds, and that predictors are on the same scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nIndependence\nNormality\nNo_Outliers\nLinearity\nHigh_Dimensionality\nFeature_Scaling\n\n\n\n\nLogistic Regression\nYes\nNo\nYes\nYes\nNo\nYes\n\n\nKNN\nNo\nNo\nNo\nNo\nYes\nYes\n\n\nNaive Bayes\nYes\nYes\nNo\nNo\nNo\nNo\n\n\nDecision Trees\nNo\nNo\nNo\nNo\nNo\nNo\n\n\nSupport Vector Machines\nNo\nNo\nNo\nNo\nNo\nYes\n\n\n\n\n\nThe above table lists the model assumptions that need to be understood and investigated further. Make sure you investigate these assumptions per research question, as different questions will use different data. If an assumption is violated, you can either transform your data to meet the assumption or eliminate that test from your analyses. Below I will show you how to run checks for independence, normality, strong outliers, linearity, high dimensionality, and how to feature scale your data.\n\n9.0.1 Independence\nFirst, we will look at independence assumption. This assumes that each observation/subject is in no way influenced by or related to the measurements of other observations/subjects. Instead of checking this assumption with a test, it is best to think carefully though how your data was collected to determine if this has been violated. Some examples of instances where independence is violated include data that are repeated measures, longitudinal, or clustered in some other way. Our data was not independent because the ManyDogs data is nested within place. The ManyDogs data includes 20 different testing sites, so subjects are related to each other based on location. For the purposes of this tutorial, we will not run tests that need the independence assumption to be satisfied; however, if you want to run a test that requires independence, there are multiple ways of transforming data or accounting for independence in a model. One example would be to take the average values of DVs within the site, and then the data could be run using logistic regression; however, this causes a great deal of loss in our data. Another way is to use mixed effect models and include the location or lab as a random effect. However, these are both outside the scope of this tutorial.\n\n\n9.0.2 Normality\nThe second assumption we have to look at is Normality. Our only test that requires normality is Naive Bayes, which assumes that continuous predictor variables are normally distributed. Normality assumes your data roughly fits a bell shape if mapped as a histogram, with most data close to the middle of the total range and few extreme values. If normality is violated, you should either not run the model that assumes normality, or transform your data to become normally distributed. There are a number of different tests that can check for normality; however, we will use the two most often used that are easily interpretable: Quantile-quantile plot (Q-Q plots) and density plots. Q-Q plots plot a continuous variable along a diagonal line. If the points on the Q-Q plot follow the line representing the normal distribution, then you have a normal distribution. If the points significantly deviate from the plotted line, you do not have a normal distribution. For a density plot, you are looking for a bell shape.\nBelow is the R code that creates a Q-Q plot and then a density plot for the continuous variable we will use in research questions 1 and 2: training_score.\n\nqqnorm(manydogs_missing_handled$training_score)\nqqline(manydogs_missing_handled$training_score)\n\n\n\n\n\n\n\nplot(density(manydogs_missing_handled$training_score), main = \"Density Plot of Training Score\")\n\n\n\n\n\n\n\n\nYou can see that the points on the Normal Q-Q Plot fall more on the line than not. If more than 25% of data doesn’t fall on the line, then it is unlikely you have normality. The line should also be symmetrical; if you have a curve in the line, you do not have normality. On the density plot, you are looking for no extra bumps on the the tails of the curve: the tails should approach 0 at a fairly constant rate. Note that the smaller the sample, the less descriptive plots can be, so it might be best to check other tests of normality if you aren’t sure after checking these plots. Normality tests include the Shapiro-Wilk test or Kolmogorov-Smirnov test. For help getting an intuitive sense of what is sufficiently normal see here for Q-Q plots, and here for density plots.\nThe above plots satisfy the normal distribution assumption. Training score is the only continuous variable in research questions 1 and 2, so we are done checking normality for these questions.\nResearch question 3, however, has multiple continuous variables so we have to check each of the continuous variables individually. Let’s make Q-Q and Density plots for each of the following continuous predictors used in RQ3: age, aggression score, attachment score, excitability score, fear score, miscellaneous score, and separation score. I’ll show you the first two, but they all have the same pattern.\n\n#Plots for Age\nage_qqplot &lt;- qqnorm(manydogs_missing_handled$age)\nqqline(manydogs_missing_handled$age)\n\n\n\n\n\n\n\nage_density &lt;- plot(density(manydogs_missing_handled$age), main = \"Density Plot of Age\")\n\n\n\n\n\n\n\n#Plots for Aggression\naggression_qqplot &lt;- qqnorm(manydogs_missing_handled$aggression_score)\nqqline(manydogs_missing_handled$aggression_score)\n\n\n\n\n\n\n\naggression_density &lt;- plot(density(manydogs_missing_handled$aggression_score), main = \"Density Plot of Aggression\")\n\n\n\n\n\n\n\n#Plots for Attachment\n#attachment_qqplot &lt;- qqnorm(manydogs_missing_handled$attachment_score)\n#qqline(manydogs_missing_handled$attachment_score)\n#attachment_density &lt;- plot(density(manydogs_missing_handled$attachment_score), main = \"Density Plot of Attachment\")\n\n#Plots for Excitability\n#excitability_qqplot &lt;- qqnorm(manydogs_missing_handled$excitability_score)\n#qqline(manydogs_missing_handled$excitability_score)\n#excitability_density &lt;- plot(density(manydogs_missing_handled$excitability_score), main = \"Density Plot of Excitability\")\n\n#Plots for Fear\n#fear_qqplot &lt;- qqnorm(manydogs_missing_handled$fear_score)\n#qqline(manydogs_missing_handled$fear_score)\n#fear_density &lt;- plot(density(manydogs_missing_handled$fear_score), main = \"Density Plot of Fear\")\n\n#Plots for Miscellaneous\n#miscellaneous_qqplot &lt;- qqnorm(manydogs_missing_handled$miscellaneous_score)\n#qqline(manydogs_missing_handled$miscellaneous_score)\n#miscellaneous_density &lt;- plot(density(manydogs_missing_handled$miscellaneous_score), main = \"Density Plot of Miscellaneous\")\n\n#Plots for Separation\n#separation_qqplot &lt;- qqnorm(manydogs_missing_handled$separation_score)\n#qqline(manydogs_missing_handled$separation_score)\n#separation_density &lt;- plot(density(manydogs_missing_handled$separation_score), main = \"Density Plot of Separation\")\n\nUnfortunately, none of the plots for our research question 3 fit the normality assumption. We know this because the density plots do not show a smooth bell curve - there are multiple peaks, instead of just 1 in the center - and graphs aren’t symmetrical. For the Q-Q plots, the data has large deviations from the normality line with much more data than 25% not touching the line. There are also deviations on the tail ends, with both tails going off in different directions. Therefore, we do not have normality and cannot run a Naive Bayes model with this data.\nFor the purpose of this tutorial, we will conclude this section here. However, there are a number of ways to transform data to try and make continuous predictors conform to normality (i.e. log transformations, square root transformations, and more). Keep in mind as you investigate how to transform your data into a normal distribution that different transformations work better on different kinds of data issues (i.e. skewed data, data with many outliers, etc.). See here for a description of different kinds of transformations.\n\n\n9.0.3 No Strong Outliers\nOur next assumption, no strong outliers, is easy to understand but hard to implement, as there is no clear rule as to when to keep and when to delete outliers. The longer you work with data, the better your decisions for this and other questions become - but until that time, there are a few general rules you can follow. If you are working with thousands of data points with low dimensionality, deleting outliers is easier because you will have more data on which to base the decision on what is an outlier not worth investigating, and what is simply a trend with few respondents on a specific range of values.\nBut if you have a small amount of data (n&lt;1000) like we do in this tutorial, it is best not to delete any observations. One exception to this rule is if you know for a fact a specific data point is incorrect or impossible, then it is okay to delete. For example, if a participant writes that they are 230 years old, you can be certain this was either a typo or error inputting the data, or they are a vampire and their responses should not be used to predict human behavior.\nThere are multiple ways to determine what is an outlier in your data. I am going to show you the one that is most widely applicable and easily understandable, the Interquartile Range (IQR) method. In this method, an outlier is considered a point that is above the 75th percentile or below the 25th percentile by a factor of 1.5 times the IQR. So, to calculate what points are outliers, it is as simple as making a boxplot. It is best to look at each of your predictor’s boxplots to investigate if the points you think are outliers are actually reasonable responses to the questions that just happen to be slightly out of bounds of the IQR, rather than incorrect or problematic data.\n\n#Create a boxplot for age\nage_boxplot &lt;- boxplot(manydogs_missing_handled$age)\n\n\n\n\n\n\n\n#Pull out the values of the outliers \nage_boxplot$out\n\n[1] 14.0 20.8\n\n\nHere you can see a visual representation of the data and what is calculated to be an outlier. The argument $out allows you to extract from the boxplot object a vector listing all the outliers. Here we see two extreme values are present, but the presence of these values alone doesn’t indicate anything is wrong, just that two points are slightly outside of the IQR. It is possible for a dog to be 20 years old, so this point is outside the IQR but not problematic.\nFeel free to make boxplots for other variables in the data; however, I have looked through all the outliers and didn’t find anything suspicious or incorrectly inputted, so for the rest of this tutorial we will leave the other extreme values alone. For non-tutorial based projects a good rule of thumb is to have at least two or possibly three people look at and discuss which outliers to keep or delete based on best practices for your discipline.\nIf you want to delete the outliers, you can simply delete the observations listed in the $out vector.\nOne last note on outliers: The Z-score method is also used to detect outliers. However, Z scoring can introduce problems as it is calculated using the mean and standard deviation. If your data are skewed (which is often the case when we are considering data with outliers), Z-scoring ends up being overly liberal with defining what constitutes an outlier. The IQR is less sensitive than Z-scoring, which has the effect of erring on the side of keeping rather than getting rid of outliers. Again, you only want to get rid of values that are the result of measurement error, data entry error, or has unusual properties that show that that point is not part of the study population. See here for more information on when to keep or take out outliers.\n\n\n9.0.4 Linearity\nLinearity is our next assumption. Linearity assumes that each predictor in the model, when holding all the other predictors in the model constant, will change in a linear way with the outcome variable. (In other words, don’t put a line through something that is not a line). Unlike in linear regression, in logistic regression, we are looking at the log odds of the probability of the outcome (i.e., being in a particular category). To diagnose if this is true or not, we need to make a graph of this relationship and see if the plot satisfies the assumption. These plots are called a component plus resistance or CR plot, which can be made with the crPlots function in the car package.\nWe will make a plot for each continuous predictor per research question. We will break code into three sections, one for each research question. You do not need to check the categorical predictors as they are always linear. To read more about why this is see here.\n\n#Basic logistic regression model with training_score predictor variable and outcome variable of whether the dog did above chance on the ostensive task\nmodel_RQ_1 &lt;- glm(ostensive_binary ~ training_score, data = manydogs_missing_handled, family = binomial)\n\ncrPlots(model_RQ_1, terms = ~training_score,\n             pch=20, col=\"gray\",\n             smooth = list(smoother=car::gamLine))\n\n\n\n\n\n\n\n\nThe dashed line on a CR plot is the relationship between the continuous predictor and the outcome variable, assuming linearity. The solid pink line represents the actual observed relationship in the data. If these two are very different, then the linearity assumption has been violated. If the two lines look similar, then linearity has been confirmed. As you can see in this plot, we do not have overlap between the two lines, so we have a deviation from linearity. When you do not have linearity you have two possible options: 1) do not run analyses that need a linear relationship in data (i.e., what we will do in this tutorial) or 2) transform the data to make it more linear. However, this isn’t possible with all data and you will have to transform and then recheck the assumption. See here for resources on how to transform nonlinear data into linear data.\nNow let’s check linearity for question 2.\n\n#Make basic logistic regression model with training_score predictor variable and outcome variable of whether the dog did above chance on the nonostensive task\nmodel_RQ_2 &lt;- glm(nonostensive_binary ~ training_score, data = manydogs_missing_handled, family = binomial)\n\ncrPlots(model_RQ_2, terms = ~training_score,\n             pch=20, col=\"gray\",\n             smooth = list(smoother=car::gamLine))\n\n\n\n\n\n\n\n\nSimilarly, in the data for research question 2, we do not have overlap between the two lines, so we have a deviation from linearity.\nFinally, for the more complicated third research question, we have more than one predictor that is continuous, so we have to make a CR plot for each predictor variable in the model holding all others variables fixed (i.e., with the same values).\n\n#Make basic logistic regression model with all predictors and outcome variable that measures whether the dog did better at the nonostensive task\n\nmodel_RQ_3 &lt;- glm(os_best ~ age + sex + gaze_follow + origin + environment + training_score + aggression_score + fear_score + separation_score + excitability_score + attachment_score + miscellaneous_score, data = manydogs_missing_handled, family = binomial)\n\nage_crplot &lt;- crPlots(model_RQ_3, terms = ~age,\n             pch=20, col=\"gray\",\n             smooth = list(smoother=car::gamLine))\n\n\n\n\n\n\n\n#training_crplot &lt;- crPlots(model_RQ_3, terms = ~training_score,\n            #pch=20, col=\"gray\",\n            #smooth = list(smoother=car::gamLine))\n\n#aggression_crplot &lt;- crPlots(model_RQ_3, terms = ~aggression_score,\n             #pch=20, col=\"gray\",\n             #smooth = list(smoother=car::gamLine))\n\n#fear_crplot &lt;- crPlots(model_RQ_3, terms = ~fear_score,\n             #pch=20, col=\"gray\",\n             #smooth = list(smoother=car::gamLine))\n\n#separation_crplot &lt;- crPlots(model_RQ_3, terms = ~separation_score,\n             #pch=20, col=\"gray\",\n             #smooth = list(smoother=car::gamLine))\n\n#excitability_crplot &lt;- crPlots(model_RQ_3, terms = ~excitability_score,\n             #pch=20, col=\"gray\",\n             #smooth = list(smoother=car::gamLine))\n\n#attachment_crplot &lt;- crPlots(model_RQ_3, terms = ~attachment_score,\n             #pch=20, col=\"gray\",\n             #smooth = list(smoother=car::gamLine))\n\n#miscellaneous_crplot &lt;- crPlots(model_RQ_3, terms = ~miscellaneous_score,\n             #pch=20, col=\"gray\",\n             #smooth = list(smoother=car::gamLine))\n\nAll eight of the plots we made for the continuous variables show near identical overlap between the two lines. Therefore, we have linearity between predictors and the log odds of the outcome variable and can move on to our next assumption! To save space and to make this tutorial less messy, I have only outputted one of the eight plots you need to check. However, when you are working through this tutorial, please look at all the plots to check this assumption.\n\n\n9.0.5 Feature Scaling\nFeature scaling or normalization is sometimes misinterpreted as needing to transform predictors to be normally distributed, which we already went over in the normality section of assumption checks. Instead, feature scaling is the process of transforming data to ensure that all the predictors in the model are being analyzed on the same scale. Feature scaling allows you to accurately compare two predictors that have different magnitudes. For instance, if you have a data set looking at what ages and income levels are associated with middle class status (binary category), certain algorithms will misclassify data points simply because the larger range on the scale of income will artificially inflate the difference between age/income and middle-class status. Feature scaling also helps algorithms converge faster and takes less processing power.\nNow that you know why we transform predictors by scaling them, let’s scale our continuous predictors. There are several ways to transform predictors that all do slightly different things to the data, but for now we will transform our data with the most commonly used transformation: z-scores. Z-scores transform a column of values into a new variable with a mean of 0 and standard deviation of 1 by subtracting the original value by the mean of the column, and dividing that new number by the standard deviation. This process makes the range and magnitude of our columns easily comparable so outcomes are more reliable and interpretable. (Note: this is different from the Z-score outlier detection we discussed above - here, Z scores are our friends!)\nTo see explanations of other types of transformations see this great guide on R pubs.\nWe will use the function scale to apply the z-score function to our continuous predictor columns. This function comes with base R, so no need to install another package.\n\nmanydogs_transformed &lt;- manydogs_missing_handled %&gt;%\n  mutate_at(vars(age, training_score, aggression_score, fear_score, separation_score, excitability_score, attachment_score, miscellaneous_score), scale)\n\nNow we can use this dataset for the rest of our analyses!",
    "crumbs": [
      "Step 1: Exploring Data & Checking Assumptions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Other Model Assumptions</span>"
    ]
  },
  {
    "objectID": "runningmodels.html",
    "href": "runningmodels.html",
    "title": "15  Running your models",
    "section": "",
    "text": "15.0.1 Creating the Task\nTo define the task we will use the makeClassifTask() which defines a classification task. If doing regression tasks, you would use the function makeRegrTask(). You then pass the data to the data argument, and the name of the predictor variables to the target argument. For our purposes, we will be defining three tasks because we have three research questions, each with slightly different data representing the different outcomes and predictors. Because we only want to pass the data that is absolutely necessary for each task, we have to make those data frames before we can use them in to create the Task.\n#Research Question #1\n#extract only the two columns from the large pre-processed training dataset we made in step 1 that are needed for the research question: training score and the ostensive binary outcome variable\ndata_RQ_1 &lt;- training_data |&gt;  \n  select(training_score, ostensive_binary) |&gt;  \n    mutate(ostensive_binary = as.factor(ostensive_binary)) \n\n#Define the Task for RQ #1\ntask_RQ_1 &lt;- makeClassifTask(data = data_RQ_1, target = \"ostensive_binary\")\n\n#Research Question #2\n#extract only the two columns from the large pre-processed dataset that are needed for the research question: training score and the nonostensive binary outcome variable\ndata_RQ_2 &lt;- training_data |&gt; \n  select(training_score, nonostensive_binary) |&gt; \n  mutate(nonostensive_binary = as.factor(nonostensive_binary)) \n\n#Define the task for RQ #2\ntask_RQ_2 &lt;- makeClassifTask(data = data_RQ_2, target = \"nonostensive_binary\")\n\n#Research Question #3\n#take out the two outcome variables that we don't want to use in this analysis but grab every other predictor\ndata_RQ_3_factor &lt;- training_data |&gt; \n  select(-c(ostensive_binary, nonostensive_binary)) |&gt; \nmutate(across(c(sex, desexed, purebred, gaze_follow, os_best), as.factor))\n\ndata_RQ_3_numeric &lt;- data_RQ_3_factor |&gt; \nmutate(across(c(sex, desexed, purebred, gaze_follow), as.numeric))\n\n#Define the task for RQ #3\ntask_RQ_3_factor &lt;- makeClassifTask(data = data_RQ_3_factor, target = \"os_best\")\n\ntask_RQ_3_numeric &lt;- makeClassifTask(data = data_RQ_3_numeric, target = \"os_best\")\nNow we have created all the tasks we will need for every model we run! That is why it is nice to define the data before you create the model. We save ourselves extra coding and make our coding more readable.",
    "crumbs": [
      "Step 4: Choose an Algorithm and Run with Hyperparameters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Running your models</span>"
    ]
  },
  {
    "objectID": "runningmodels.html#generic-logistic-regression-example",
    "href": "runningmodels.html#generic-logistic-regression-example",
    "title": "15  Running your models",
    "section": "15.1 Generic Logistic Regression Example",
    "text": "15.1 Generic Logistic Regression Example\nWhile the current data did not call for us to use logistic regression, it is still a useful algorithm to know how to use for many potential problems. So, here is code that you can use to run your own logistic regression.\nWe will use the task we created for research question 1. We will have to make a new learner to define our model using the classif.logreg argument for logistic regression. Set the argument predict.type equal to prob if you want the estimated probabilities of each class outputted, as well as what class each observation is predicted to be in. Then all you have to do is put it together and cross validate:\n\nLR_learner_RQ_1 &lt;- makeLearner(\"classif.logreg\", predict.type = \"prob\")\n\nLR_model_RQ_1 &lt;- suppressMessages({ \n  train(LR_learner_RQ_1, task_RQ_1)\n})\n\nLR_model_RQ_1_loocv &lt;- suppressMessages({ \n  resample(learner = LR_learner_RQ_1, \n                       task = task_RQ_1,\n                       resampling = loocv)\n})\n\nLR_model_RQ_1_loocv$aggr\n\nmmce.test.mean \n     0.2841918 \n\n\nAs predicted, this model did NOT do a good job at predicting the correct category for our dogs.",
    "crumbs": [
      "Step 4: Choose an Algorithm and Run with Hyperparameters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Running your models</span>"
    ]
  },
  {
    "objectID": "runningmodels.html#generic-naive-bayes-example",
    "href": "runningmodels.html#generic-naive-bayes-example",
    "title": "15  Running your models",
    "section": "15.2 Generic Naive Bayes Example",
    "text": "15.2 Generic Naive Bayes Example\nAs with logistic regression, we had decided not to use Naive Bayes for this example dataset, but having some base code may be handy for those cases where it would be appropriate. Naive Bayes algorithms only have one hyperparameter to tune, laplace. You typically want to add a small value to laplace, as it is just a placeholder that adds that value to all of the probabilities to handle the issue of a zero probability for some features. Unseen feature values (0s) are assigned a small, non-zero probability, which helps prevent zero probabilities from making your probabilities be artificially low (as anything multiplied by 0 is 0), and improves the robustness of the Naive Bayes classifier.\nI will use the task_RQ_1 task and we have to make the learner, train the model, and cross validate.\n\nNB_learner_RQ_1 &lt;- makeLearner(\"classif.naiveBayes\", par.vals = list(\"laplace\" = 1))\n  \nNB_model_RQ_1 &lt;- train(learner = NB_learner_RQ_1, \n                       task = task_RQ_1)\n\nNB_model_RQ_1_loocv &lt;- suppressMessages({ \n  resample(learner = NB_learner_RQ_1, \n                       task = task_RQ_1,\n                       resampling = loocv)\n})\n\nNB_model_RQ_1_loocv$aggr\n\nmmce.test.mean \n     0.2841918 \n\n\nYou’ll notice that we are at about 71.6% accuracy which is less predictive than most of our models. But this and all our other models haven’t been tested with new data - let’s do that now!",
    "crumbs": [
      "Step 4: Choose an Algorithm and Run with Hyperparameters",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Running your models</span>"
    ]
  },
  {
    "objectID": "step5.html",
    "href": "step5.html",
    "title": "16  Step 5: Compare Model Outcomes",
    "section": "",
    "text": "So far, we know how the models worked on the training data, but we haven’t fed our models any of our held-out testing data. To compare how each of the models did against each other we need to look at how each model does on new unseen data. We will do this by looking at prediction accuracy and (poorly named!) confusion matrices. A confusion matrix is far from confusing: it is simply a 4 by 4 table showing the false positive, false negative, true positive, and true negative rates that were produced by a specific model:\nNow that we know how to read a confusion matrix, let’s make the testing data frames we need to feed into the models.\n\n#Research Question #1\n#extract only the two columns from the large pre-processed testing dataset we made in step 1 that are needed for the research question: training score and the ostensive binary outcome variable\ntesting_data_RQ_1 &lt;- testing_data |&gt; \n  select(training_score, ostensive_binary) |&gt; \n      mutate(training_score = as.numeric(training_score),\n         ostensive_binary = as.factor(ostensive_binary))\n\n#Research Question #2\n#extract only the two columns from the large pre-processed dataset that are needed for the research question: training score and the nonostensive binary outcome variable\ntesting_data_RQ_2 &lt;- testing_data |&gt; \n  select(training_score, nonostensive_binary) |&gt; \n    mutate(training_score = as.numeric(training_score),\n         nonostensive_binary = as.factor(nonostensive_binary))\n\n#Research Question #3\n#take out the two outcome variables that we don't want to use in this analysis but leave every other predictor\ntesting_data_RQ_3_factor &lt;- testing_data |&gt; \n  select(-c(ostensive_binary, nonostensive_binary)) |&gt; \nmutate(across(c(sex, desexed, purebred, gaze_follow, os_best), as.factor))\n\nNow that we have our testing data for each of the three research questions, we can use the two functions in mlr that help us look at model outcomes: predict() and performance(). predict() lets us feed a model and new data into the function to get the predicted classification for each observation in the testing data. You then feed the object you made with predict() into performance() to get the confusion matrix information. Cool, right? Let’s do it!\n\n#predicting new values and getting performance metrics for all 3 models run with RQ 1\n#KNN model\nknn_predictions_RQ1 &lt;- predict(KNN_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(knn_predictions_RQ1)\n\n     mmce \n0.2553191 \n\ncalculateConfusionMatrix(knn_predictions_RQ1)\n\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       36 0     36\n  -err.-  36 0     36\n\n\nThe KNN model is classifying 75% of the cases correctly, which sounds okay. However, when we look further into the confusion matrix, we can see that the algorithm is classifying almost every observation as 0 (i.e., that almost every dog was performing below chance at the ostensive task, which we know isn’t correct). So, this algorithm is probably not a good one to use when understanding the true relationship between the predictor and variable.\n\n#Decision Tree\ndecision_tree_predictions_RQ1 &lt;- predict(decision_tree_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(decision_tree_predictions_RQ1)\n\n     mmce \n0.2553191 \n\ncalculateConfusionMatrix(decision_tree_predictions_RQ1)\n\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       36 0     36\n  -err.-  36 0     36\n\n\nThe decision tree model did even worse than our simplest model as it predicted every observation would be 0. Again, not very useable.\n\n#SVM\nSVM_predictions_RQ_1 &lt;- predict(SVM_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(SVM_predictions_RQ_1)\n\n    mmce \n0.248227 \n\ncalculateConfusionMatrix(SVM_predictions_RQ_1)\n\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       35 1     35\n  -err.-  35 0     35\n\n\nThe SVM algorithm has the same results as our decision tree even though it took 10 times as long to train. Again, not very useable.\nNow let’s do the same thing for research questions 2.\n\n#predicting new values and getting performance metrics for all 3 models run with RQ 2\n#KNN model\nknn_predictions_RQ2 &lt;- predict(KNN_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(knn_predictions_RQ2)\n\n     mmce \n0.3546099 \n\ncalculateConfusionMatrix(knn_predictions_RQ2)\n\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n\n\nThis KNN model performed worse than RQ 1, but had the same issue in that it predicted everything would be 0. (These models really don’t think much of our dog’s intelligence!)\n\n#Decision Tree\ndecision_tree_predictions_RQ2 &lt;- predict(decision_tree_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(decision_tree_predictions_RQ2)\n\n     mmce \n0.3546099 \n\ncalculateConfusionMatrix(decision_tree_predictions_RQ2)\n\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n\n\nThis decision tree did just as bad as the KNN.\n\n#SVM\nSVM_predictions_RQ2 &lt;- predict(SVM_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(SVM_predictions_RQ2)\n\n     mmce \n0.3546099 \n\ncalculateConfusionMatrix(SVM_predictions_RQ2)\n\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n\n\nThis algorithm did slightly better than the KNN model but still had similar issues.\nNow let’s do the same thing for research question 3!\n\n#predicting new values and getting performance metrics for all 3 models run with RQ 2\n#KNN model\nknn_predictions_RQ3 &lt;- predict(KNN_model_RQ_3, newdata = testing_data_RQ_3_factor)\n\nperformance(knn_predictions_RQ3)\n\n     mmce \n0.3049645 \n\n#Dig deeper into predictions with a confusion matrix\ncalculateConfusionMatrix(knn_predictions_RQ3)\n\n        predicted\ntrue      0 1 -err.-\n  0      98 0      0\n  1      43 0     43\n  -err.- 43 0     43\n\n\nUnlike in RQ 1 and 2, the algorithm now is predicted that everything will be 1 (i.e., that all dogs will perform better at the nonostensive task than the ostensive, which doesn’t fit what we know about how dogs use human cueing). This is also not a very helpful algorithm to use in the future.\n\n#Random Forest\nrandomforest_predictions_RQ3 &lt;- predict(random_forest_model_RQ_3, newdata = testing_data_RQ_3_factor)\n\n#Measure how well the model did at predictions\nperformance(randomforest_predictions_RQ3)\n\n      mmce \n0.03546099 \n\n#Dig deeper into predictions with a confusion matrix\ncalculateConfusionMatrix(randomforest_predictions_RQ3)\n\n        predicted\ntrue      0  1 -err.-\n  0      93  5      5\n  1       0 43      0\n  -err.-  0  5      5\n\n\nThe random forest algorithm has the exact same results as the KNN.",
    "crumbs": [
      "Step 5: Assess Model Performance",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Step 5: Compare Model Outcomes</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "17  Concluding Insights",
    "section": "",
    "text": "What we have found out is that none of the predictors in the models were able to helpfully predict the outcome on the tasks. This means one of two things: 1) That we need to collect more data to understand the relationship between all these variables and the outcome behavioral task or 2) That none of the collected attributes predict the outcome well. For this specific data set no algorithm can get better results than just guessing the most common outcome for every subject. This is because this dataset is highly skewed. This example illustrates an important lesson in the importance of having symmetrically distributed outcome variables.\nYou will also notice that there wasn’t a significant increase in prediction accuracy between the simplest models and the most complicated (computationally expensive) models. This is true across most machine learning problems. Often the simplest models do a good job at predicting and more complicated models don’t increase predictive accuracy a huge amount. The more hyperparameters you tune and the more cross-validation loops you add, the more you can usually expect a slight increase in predictive accuracy. So when you are engaging in these practices yourself its good to temper expectations with wildly improving models with increased computing power. You typically get diminishing returns when adding complexity. After a certain point adding complexity is only causing your model to overfit to the current data set and therefore perform poorly on new data.\nWait, so after all that work what do we have? While we dealt with missing data, completed all necessary assumption checks per model, ran hyperparameter tuning to help fitting, and ran multiple algorithms…what we eventually got was a bunch of models that performed poorly and don’t have good spread across prediction. This is an extremely important lesson: machine learning isn’t a magic wand that will generate significant effects out of thin air. Not all datasets are predictive for every research question! Machine learning can’t replace collecting lots of good representative data, or asking appropriate research questions – that’s up to you. Luckily, you now have the skills to apply classification machine learning to a new question.\nI hoped this tutorial was helpful! If you want to learn more about any of the concepts I referenced in this tutorial please look through my references section as well as the linked websites throughout. The three textbooks I used for this tutorial were invaluable to my progress in machine learning and all three can be accessed free online. Happy Predicting!\n\nYOU DID IT!!!",
    "crumbs": [
      "Conclusion and Supporting Information",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Concluding Insights</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "18  Glossary of Terms",
    "section": "",
    "text": "18.1 Package Versions\nBelow I have listed the R and package versions I am using. If you are reading this tutorial in the future after updates have broken code, you can revert back.",
    "crumbs": [
      "Conclusion and Supporting Information",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Glossary of Terms</span>"
    ]
  },
  {
    "objectID": "glossary.html#package-versions",
    "href": "glossary.html#package-versions",
    "title": "18  Glossary of Terms",
    "section": "",
    "text": "R Version\n\n4.3.3\n\ntidyverse\n\nVersion 2.0.0\n\nknitr\n\nVersion 1.46\n\nhere\n\nVersion 1.0.1\n\njanitor\n\nVersion 2.2.0\n\nggpubr\n\nVersion 0.6.0\n\ncar\n\nVersion 3.1.2\n\nknitr\n\nVersion 1.46\n\nmlr\n\nVersion 2.19.1\n\nparallelMap\n\nVersion 1.5.1\n\nparallel\n\nVersion 4.3.3\n\nrandomForest\n\nVersion 4.7.1.1",
    "crumbs": [
      "Conclusion and Supporting Information",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Glossary of Terms</span>"
    ]
  },
  {
    "objectID": "step2.html",
    "href": "step2.html",
    "title": "12  Step 2: Randomly Partition the Data into Training and Test Sections",
    "section": "",
    "text": "In this step, we will perform a basic cross validation and set our randomization component. Cross validation occurs when you partition the data into sections or parts. To begin, we must partition that data into two parts: a training and a testing set. You want to fit your models on the training data, and then use the testing data to assess how well the model can predict categories based on new (unseen) data. If you skip this step, you will run into an issue known as data leakage. Data leakage occurs when data from outside the training data is used to build a predictive model. If you were to build the model on all the data and then use a subsection of that to test it, the results you get would not reflect how well that model would predict newly collected data. That model would be too reliant on the specific data you have instead of the truth of the relationship between the variables in the real world, also known as overfitting.\n\nset.seed(1234567) # ensures randomized outputs are reproducible\n\ntrain_indices &lt;- sample(1:nrow(manydogs_transformed), 0.8 * nrow(manydogs_transformed)) #Grabs a random subset of the data. In this case we asked for 80% of the data to be sampled which is the standard\ntraining_data &lt;- manydogs_transformed[train_indices, ] #Grab the 80% of the data you just pulled out and call it training\ntesting_data &lt;- manydogs_transformed[-train_indices, ] # Grab the other 20% not in the train indices and call it testing",
    "crumbs": [
      "Step 2: Randomly Partition the Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Step 2: Randomly Partition the Data into Training and Test Sections</span>"
    ]
  },
  {
    "objectID": "step3.html",
    "href": "step3.html",
    "title": "13  Step 3: Repeated Cross Validation",
    "section": "",
    "text": "Repeated cross validation is a little like data Russian nesting dolls. It is also simpler than most people expect it to be. Repeated cross validation simply refers to the concept of breaking your data into smaller sections to get an average of how well a model does on different subsets of the data. That way you have an average value for whatever you are interested in - whether than be your loss/cost function or predictive accuracy. An average allows a more accurate estimate for outcomes than relying on one partition of the data alone. If you only cut the data once, there is a likelihood that the training partition you randomly selected could have a disproportionately high number of outliers, or didn’t have a good distribution of observation values. Cross validation is a way of making the randomization component of training a dataset less subject to chance.\n\n\n\n\n\n\n\n\n\nThe three main types of cross validation are k-fold, leave one out, and nested cross validation. You use them in the following instances:\n\nLeave one out cross validation (LOOCV) - best to use when your dataset is very small (n ~ 500).\nK-fold cross validation - usually best to use when your dataset is large (n &gt; 1000). The K stands for any number you would like, usually 10-fold cross validation. K stands for how many times you would like the data to be partitioned.\nNested cross validation - best to use when you need to tune hyperparameters like k in KNN models or lambda in regularization techniques.\n\nThe value of 1000 observations as a small cutoff isn’t a universal standard. Instead, what constitutes large or small means different things to different people depending on the field, the type of model you are running, and if you need to tune hyperparameters. Yes, this is a vague answer for what is small and what is large. It’s also the most accurate answer. But, to help you out: In psychology, most experiments not involving neuroimaging data from large consortiums or genome-wide association studies are usually considered “small” in the data science world. Large language learning models can have billions of observations.\nNow, let’s break down these three types of cross validation to explain them further! All three cross validation options simply cut the data you are working on into small chunks. Depending on which cross validation technique you choose, they will complete slightly different cuts (partitions) to subdivide the data.\nLeave one out cross validation iteratively runs as many models as you have observations using all but one observation each time to train the model and then uses the one observation it left out to test the model. It does this until every observation has been used as the testing data. This is best for very small datasets (n &lt; 1000), because when you get up to the thousands of observations the computing power required gets prohibitively expensive and time consuming.\nK-fold cross validation cuts the data into k sections with k being any positive integer you like. The smallest value k can be is 2, and the largest option being the number of observations. If your k is the number of observations, you have reinvented Leave one out cross validation! (Fun, right?) Once the data is broken into k sections it then runs as many models as the number k iteratively until all folds have been used as the validation/testing set.\nNested cross validation runs two different cross validations with one within the other (like our nesting dolls!). The outer loop of a nested cross validation runs exactly like a k fold cross validation. The inner fold also runs exactly like a k fold cross validation, but instead of running on the entire data set but leaving 1 fold out, it runs on each fold of the outer loop’s training set. Each loop run in the inner loop uses a different set of hyperparameters, like choosing what value of lambda to use in regularization or what k to use in a KNN model. When the inner loop concludes it can tell you what the optimal hyperparameter is based on which value gives the lowest loss/cost function. The outerloop then runs using the chosen hyperparameter to return the best performing model given your data.\nThere are a few other types of cross validation, but they are all variations on cutting things into parts and running small data sections through the model. The more computing power and time you have access to, the fancier you can make your cross-validation procedure. I won’t go into detail on some of the more complicated cross validation practices as the fancier ones do best on very large datasets in the millions where all the cutting/partitioning can still be meaningful. If your data, like in this tutorial’s example, has less than 1000 observations, it is less likely that increasing the complexity of your cross validation would be meaningful, helpful, or worth the time and computing power.\nFor our purposes in this tutorial, our training data set only has 563 observations (which is considered small in machine learning), so we will be using leave one out cross validation (LOOCV). LOOCV is not the only or “perfect” method to use with this or your dataset. There is more than one way to skin a cat and many ways to partition a dataset.\nThere are multiple ways in R to create code to run a cross validation. We will use the makeResampleDesc() function from the mlr package to define what cross validation procedure you are completing. The mlr (machine learning in R) package came out in 2016 and includes a suite of tools to create machine learning models quickly and efficiently in R. This package is the main package we will be using in this tutorial. All three questions we are investigating use the same size dataset, so we will use LOOCV for all three. I have also shown you in a comment how you would define a 10 fold cross validation, as that is the most often used cross validation method. In the comment below this is repeated 10 times with the reps argument resulting in the model being ran a hundred times.\n\nloocv &lt;- makeResampleDesc(method = \"LOO\") #define parameters for cross validation\n\n#10fold_cross_validation &lt;- makeResampleDesc(method = \"RepCV\", folds = 10, reps = 10, stratify = TRUE) #If you want a different number of folds you can change the number to anything you like. If your number of folds is the same number as your observations than you have remade LOOCV!\n\nAbove is the only code you need for now. When you run your model, you will set the resampling argument to loocv. We will add loocv to our models when we actually run the model, but to show you what it will look like, here is some dummy code:\n\n#model &lt;- resample(learner = knn, task = data, resampling = loocv)",
    "crumbs": [
      "Step 3: Run Repeated Cross Validation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Step 3: Repeated Cross Validation</span>"
    ]
  },
  {
    "objectID": "step4.html",
    "href": "step4.html",
    "title": "14  Step 4: Choosing Algorithms & Running Models",
    "section": "",
    "text": "14.1 Choosing an Algorithm\nThroughout this tutorial so far, I have introduced you to 5 main model types: Logistic regression, K nearest neighbors, Naive Bayes, Decision trees/Random forest, and Support vector machines. Let’s go through a brief description of what each model does some of the pros and cons of each, as well as what type of predictor and outcome variables each can handle. (I do not plan to explain each algorithm to such depth that you understand the math. That is beyond the scope of this tutorial but there is a linked video describing each algorithm type in detail which gives you an amazing starting point.)",
    "crumbs": [
      "Step 4: Choose an Algorithm and Run with Hyperparameters",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Step 4: Choosing Algorithms & Running Models</span>"
    ]
  },
  {
    "objectID": "step4.html#choosing-an-algorithm",
    "href": "step4.html#choosing-an-algorithm",
    "title": "14  Step 4: Choosing Algorithms & Running Models",
    "section": "",
    "text": "Logistic Regression - Logistic regression fits a S shaped squiggle to data.\n\n\nPros: it is easily interpretable, and it doesn’t assume normally distributed data.\nCons: It won’t work if there is complete separation between the two classes (i.e., if there is no overlap on the graph). But it also won’t work if there is near total overlap.\nOutcome variable type: Takes only binary categorical outcome variables\nPredictor variable type: Takes both categorical and continuous predictor variables\n\n\nK nearest neighbors(KNN) - KNN algorithms assign unknown data to a category based on what category its neighbors are listed as. For example, if you specify that k is 3, it will look at the classifications of the three closest points to the new data and tell you that your new data is the category that is the average value of the three points. If all three points belong to one category, your new point belongs to that category. If two points belong to one category it will also tell you that your new value belongs to that category.\n\n\nPros: Simple to understand, makes no assumptions about data\nCons: Can get very computationally expensive to run if you have large datasets; accuracy is significantly impacted by noisy data and outliers; in high-dimensional datasets it performs badly; and categorical data needs to be recoded into numerical to run.\nOutcome variable type: Takes only categorical outcome variables\nPredictor variable type: Takes both categorical and continuous predictor variables\n\n\nNaive Bayes - Multiples the probabilities of each feature being found in a particular dataset. For example, if the probability of a dog being 2 years old is 6/10 for the correct choice and 2/10 for the incorrect choice, those probabilities are multiplied with all other probabilities for each variable, and the category with the higher value per observation tells the computer which bin to classify the observation in.\n\n\nPros: Doesn’t need to be tuned for hyperparameters; computationally inexpensive; can handle minimal amount of missing data if need be (good rule of thumb is less than 5% per column); works best on classifying based on words\nCons: Assumes predictor variables are normally distributed; assumes predictors are independent and suffers a lot when they aren’t\nOutcome variable type: Takes only categorical outcome variables\nPredictor variable type: Takes both categorical and continuous predictor variables\n\n\nDecision Tress/Random forest}{#decisiontrees} - A decision tree is a flow chart made by the algorithm that follows a line of logic to its conclusion. You start at the root and you go down a branch based on a feature of the data (i.e. high or low score on training) until you reach a leaf (bin) that has your final guess for which category a new datapoint should belong to. Node’s are any point between the start of the tree and a leaf where the algorithm makes a decision to move from branch to branch to leaf. Each node (decision point) partitions predictors based on values that best categorize outcome variables.\n\n\nPros: Flexible; easily interpretable; no assumptions; more robust to missing data than most other algorithms (good rule of thumb is no more than 10% per column though as predictive accuracy gets worse the more missingness you have); good with outliers. Running multiple decesion trees on different subsections of the data and averaging prediction outcomes is called a random forest. This is easy to remember as many trees creates a forest.\nCons: Individual trees WILL most likely overfit data, so they are rarely used. Instead, I have grouped decision trees with how they are most often used: in a random forest.\nOutcome variable type: Takes both categorical and continuous outcome variables\nPredictor variable type: Takes both categorical and continuous predictor variables; for random forest you need many predictors to make the forest necessary.\n\n\nSupport Vector Machines - Fits the best line possible to data that separates the categories. However, unlike linear or logistic regression it checks all kinds of lines whether that be linear (regular line), polynomial (curved line), sigmoid (S shape), or radial (circle).\n\n\nPros: Doesn’t assume independence in predictors so can be used if you have collinearity or interdependence; performs well on a wide variety of tasks; makes very few assumptions; often works very well on complex nonlinear data structures\nCons: Very computationally and time intensive to train, has many hyperparameters that all have to be tuned\nOutcome variable type: Can be used for both categorical and continuous outcomes but mostly used for categorical\nPredictor variable type: Only takes continuous predictors",
    "crumbs": [
      "Step 4: Choose an Algorithm and Run with Hyperparameters",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Step 4: Choosing Algorithms & Running Models</span>"
    ]
  },
  {
    "objectID": "step4.html#assessing-algorithms-based-on-satisfied-assumptions",
    "href": "step4.html#assessing-algorithms-based-on-satisfied-assumptions",
    "title": "14  Step 4: Choosing Algorithms & Running Models",
    "section": "14.2 Assessing Algorithms Based on Satisfied Assumptions",
    "text": "14.2 Assessing Algorithms Based on Satisfied Assumptions\nNow that we know what assumptions are met and how each model deals with data, we can now decide what models to run with each of our three research questions.\nAs a reminder our research questions are: 1. Does the amount of training a dog has completed predict whether a dog will perform under or over chance on the ostensive behavioral task? 2. Does the amount of training a dog has completed predict whether a dog will perform under or over chance on the nonostensive behavioral task? 3. Which dog characteristics predict whether a dog is more likely to perform better at the nonostensive than ostensive behavioral task?\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nLogistic\nKNN\nNaive_bayes\nDecision_trees\nSupport_Vector_Machines\n\n\n\n\nResearch Question 1\nDo Not Run\nRun\nDo Not Run\nRun Decision Tree\nRun\n\n\nResearch Question 2\nDo Not Run\nRun\nDo Not Run\nRun Decision Tree\nRun\n\n\nResearch Question 3\nDo Not Run\nRun\nDo Not Run\nRun Random Forest\nDo Not Run\n\n\n\n\n\nWe will not run logistic regression because even though our predictors and outcomes work with the model, we have violated independence and linearity (linearity was only violated for question 1 & 2 but not 3). Independence could be corrected by adding a grouping structure for lab/location but that is outside the scope of the current tutorial. Linearity could be fixed with transformations, similar to normality violations. It is best practice to start fixing the issue with the biggest problem, then recheck all of your assumptions. Do not fix multiple problems without first seeing how each individual change effects data attributes.\nKNN models can be run for all of the research question because the assumptions are met and the predictors and outcome variables match what a KNN model needs.\nNaive Bayes will not be run for any of the models because the independence assumption is violated and cannot be easily worked around by transforming variables or taking into account grouping structure. Furthermore, the normality assumption of the continuous predictor variables is violated for research question 3.\nRandom forest models could be run in theory for all the questions, because the assumptions are met and the predictors and outcome variables match what the model needs. However, running a random forest with only one continuous predictor is a waste of computational power as it won’t give you anything meaningful with so little data. (Remember, random forest models are built by making a large number of individual trees and averaging the results). You could run just one decision tree, which is what we will do below. Running an entire random forest model with one predictor is sort of like using a sharpened chefs knife to saw a piece of wood in half- it’ll get the job done but it’ll take forever and you’ve wasted a very good knife.\nSupport vector machines will be run for research questions 1 and 2, but not 3. The assumptions are met but, SVM can only handle continuous predictor variables. We have a number of variables we believe to be important that are categorical in nature, so we will not run a SVM model for research question 3.wh",
    "crumbs": [
      "Step 4: Choose an Algorithm and Run with Hyperparameters",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Step 4: Choosing Algorithms & Running Models</span>"
    ]
  }
]