{
  "hash": "bdc76c6a6f68e8c85e85a65e7d438be6",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n\n\n\n\n# Step 5: Compare Model Outcomes\n\nSo far, we know how the models worked on the training data, but we haven't fed our models any of our held-out testing data. To compare how each of the models did against each other we need to look at how each model does on new *unseen* data. We will do this by looking at prediction accuracy and (poorly named!) [confusion matrices](https://blog.roboflow.com/what-is-a-confusion-matrix/). A confusion matrix is far from confusing: it is simply a 4 by 4 table showing the false positive, false negative, true positive, and true negative rates that were produced by a specific model:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we know how to read a confusion matrix, let's make the testing data frames we need to feed into the models.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Research Question #1\n#extract only the two columns from the large pre-processed testing dataset we made in step 1 that are needed for the research question: training score and the ostensive binary outcome variable\ntesting_data_RQ_1 <- testing_data |> \n  select(training_score, ostensive_binary) |> \n      mutate(training_score = as.numeric(training_score),\n         ostensive_binary = as.factor(ostensive_binary))\n\n#Research Question #2\n#extract only the two columns from the large pre-processed dataset that are needed for the research question: training score and the nonostensive binary outcome variable\ntesting_data_RQ_2 <- testing_data |> \n  select(training_score, nonostensive_binary) |> \n    mutate(training_score = as.numeric(training_score),\n         nonostensive_binary = as.factor(nonostensive_binary))\n\n#Research Question #3\n#take out the two outcome variables that we don't want to use in this analysis but leave every other predictor\ntesting_data_RQ_3_factor <- testing_data |> \n  select(c(sex:miscellaneous_score, nonos_best)) |> \nmutate(across(c(sex, desexed, purebred, gaze_follow, nonos_best), as.factor))\n```\n:::\n\n\n\n\n\n\nNow that we have our testing data for each of the three research questions, we can use the two functions in `mlr` that help us look at model outcomes: `predict()` and `performance()`. `predict()` lets us feed a model and new data into the function to get the predicted classification for each observation in the testing data. You then feed the object you made with `predict()` into `performance()` to get the confusion matrix information. Cool, right? Let's do it!\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predicting new values and getting performance metrics for all 3 models run with RQ 1\n#KNN model\nknn_predictions_RQ1 <- predict(KNN_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(knn_predictions_RQ1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.2553191 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(knn_predictions_RQ1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       36 0     36\n  -err.-  36 0     36\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe KNN model is classifying 75% of the cases correctly, which sounds okay. However, when we look further into the confusion matrix, we can see that the algorithm is classifying every observation as 0 (i.e., that almost every dog was performing below chance at the ostensive task, which we know isn't correct). So, this algorithm is probably not a good one to use when understanding the true relationship between the predictor and variable.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Decision Tree\ndecision_tree_predictions_RQ1 <- predict(decision_tree_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(decision_tree_predictions_RQ1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.2553191 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(decision_tree_predictions_RQ1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       36 0     36\n  -err.-  36 0     36\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe decision tree model did just as poorly as the simplest model as it predicted *every* observation would be 0. Again, not very useable.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SVM\nSVM_predictions_RQ_1 <- predict(SVM_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(SVM_predictions_RQ_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    mmce \n0.248227 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(SVM_predictions_RQ_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       35 1     35\n  -err.-  35 0     35\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe SVM algorithm has the same results as our decision tree except for 1 data point even though it took 10 times as long to train. Again, not very useable.\n\nNow let's do the same thing for research questions 2.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predicting new values and getting performance metrics for all 3 models run with RQ 2\n#KNN model\nknn_predictions_RQ2 <- predict(KNN_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(knn_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3546099 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(knn_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThis KNN model had the same issue in that it predicted everything would be 0. (These models really don't think much of our dog's intelligence!)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Decision Tree\ndecision_tree_predictions_RQ2 <- predict(decision_tree_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(decision_tree_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3546099 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(decision_tree_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThis decision tree did just as bad as the KNN.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SVM\nSVM_predictions_RQ2 <- predict(SVM_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(SVM_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3546099 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(SVM_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThis algorithm had similar issues to all our other models.\n\nNow let's do the same thing for research question 3!\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predicting new values and getting performance metrics for all 3 models run with RQ 2\n#KNN model\nknn_predictions_RQ3 <- predict(KNN_model_RQ_3, newdata = testing_data_RQ_3_factor)\n\nperformance(knn_predictions_RQ3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3049645 \n```\n\n\n:::\n\n```{.r .cell-code}\n#Dig deeper into predictions with a confusion matrix\ncalculateConfusionMatrix(knn_predictions_RQ3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      98 0      0\n  1      43 0     43\n  -err.- 43 0     43\n```\n\n\n:::\n:::\n\n\n\n\n\n\nAgain, the algorithm is predicted that everything will be 0 (i.e., that all dogs will perform better at the ostensive task than the nonostensive). This is also not a very helpful algorithm to use in the future.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Random Forest\nrandomforest_predictions_RQ3 <- predict(random_forest_model_RQ_3, newdata = testing_data_RQ_3_factor)\n\n#Measure how well the model did at predictions\nperformance(randomforest_predictions_RQ3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3049645 \n```\n\n\n:::\n\n```{.r .cell-code}\n#Dig deeper into predictions with a confusion matrix\ncalculateConfusionMatrix(randomforest_predictions_RQ3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      98 0      0\n  1      43 0     43\n  -err.- 43 0     43\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe random forest algorithm has the exact same results as the KNN.\n",
    "supporting": [
      "step5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}