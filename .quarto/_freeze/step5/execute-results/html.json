{
  "hash": "634c8a95a6cba2ac406d0cb6f9d8e0ad",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n\n\n\n\n\n\n\n\n# Step 5: Compare Model Outcomes\n\nSo far, we know how the models worked on the training data, but we haven't fed our models any of our held-out testing data. To compare how each of the models did against each other we need to look at how each model does on new *unseen* data. We will do this by looking at prediction accuracy and (poorly named!) [confusion matrices](https://blog.roboflow.com/what-is-a-confusion-matrix/). A confusion matrix is far from confusing: it is simply a 4 by 4 table showing the false positive, false negative, true positive, and true negative rates that were produced by a specific model:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we know how to read a confusion matrix, let's make the testing data frames we need to feed into the models.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Research Question #1\n#extract only the two columns from the large pre-processed testing dataset we made in step 1 that are needed for the research question: training score and the ostensive binary outcome variable\ntesting_data_RQ_1 <- testing_data |> \n  select(training_score, ostensive_binary) |> \n      mutate(training_score = as.numeric(training_score),\n         ostensive_binary = as.factor(ostensive_binary))\n\n#Research Question #2\n#extract only the two columns from the large pre-processed dataset that are needed for the research question: training score and the nonostensive binary outcome variable\ntesting_data_RQ_2 <- testing_data |> \n  select(training_score, nonostensive_binary) |> \n    mutate(training_score = as.numeric(training_score),\n         nonostensive_binary = as.factor(nonostensive_binary))\n\n#Research Question #3\n#take out the two outcome variables that we don't want to use in this analysis but leave every other predictor\ntesting_data_RQ_3_factor <- testing_data |> \n  select(-c(ostensive_binary, nonostensive_binary)) |> \nmutate(across(c(sex, desexed, purebred, gaze_follow, os_best), as.factor))\n```\n:::\n\n\n\n\n\n\n\n\n\n\nNow that we have our testing data for each of the three research questions, we can use the two functions in `mlr` that help us look at model outcomes: `predict()` and `performance()`. `predict()` lets us feed a model and new data into the function to get the predicted classification for each observation in the testing data. You then feed the object you made with `predict()` into `performance()` to get the confusion matrix information. Cool, right? Let's do it!\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predicting new values and getting performance metrics for all 3 models run with RQ 1\n#KNN model\nknn_predictions_RQ1 <- predict(KNN_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(knn_predictions_RQ1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.2553191 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(knn_predictions_RQ1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       36 0     36\n  -err.-  36 0     36\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThe KNN model is classifying 75% of the cases correctly, which sounds okay. However, when we look further into the confusion matrix, we can see that the algorithm is classifying almost every observation as 0 (i.e., that almost every dog was performing below chance at the ostensive task, which we know isn't correct). So, this algorithm is probably not a good one to use when understanding the true relationship between the predictor and variable.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Decision Tree\ndecision_tree_predictions_RQ1 <- predict(decision_tree_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(decision_tree_predictions_RQ1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.2553191 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(decision_tree_predictions_RQ1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       36 0     36\n  -err.-  36 0     36\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThe decision tree model did even worse than our simplest model as it predicted *every* observation would be 0. Again, not very useable.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SVM\nSVM_predictions_RQ_1 <- predict(SVM_model_RQ_1, newdata = testing_data_RQ_1)\n\nperformance(SVM_predictions_RQ_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    mmce \n0.248227 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(SVM_predictions_RQ_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue       0 1 -err.-\n  0      105 0      0\n  1       35 1     35\n  -err.-  35 0     35\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThe SVM algorithm has the same results as our decision tree even though it took 10 times as long to train. Again, not very useable.\n\nNow let's do the same thing for research questions 2.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predicting new values and getting performance metrics for all 3 models run with RQ 2\n#KNN model\nknn_predictions_RQ2 <- predict(KNN_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(knn_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3546099 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(knn_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThis KNN model performed worse than RQ 1, but had the same issue in that it predicted everything would be 0. (These models really don't think much of our dog's intelligence!)\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Decision Tree\ndecision_tree_predictions_RQ2 <- predict(decision_tree_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(decision_tree_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3546099 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(decision_tree_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThis decision tree did just as bad as the KNN.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#SVM\nSVM_predictions_RQ2 <- predict(SVM_model_RQ_2, newdata = testing_data_RQ_2)\n\nperformance(SVM_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3546099 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalculateConfusionMatrix(SVM_predictions_RQ2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      91 0      0\n  1      50 0     50\n  -err.- 50 0     50\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThis algorithm did slightly better than the KNN model but still had similar issues.\n\nNow let's do the same thing for research question 3!\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#predicting new values and getting performance metrics for all 3 models run with RQ 2\n#KNN model\nknn_predictions_RQ3 <- predict(KNN_model_RQ_3, newdata = testing_data_RQ_3_factor)\n\nperformance(knn_predictions_RQ3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     mmce \n0.3049645 \n```\n\n\n:::\n\n```{.r .cell-code}\n#Dig deeper into predictions with a confusion matrix\ncalculateConfusionMatrix(knn_predictions_RQ3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0 1 -err.-\n  0      98 0      0\n  1      43 0     43\n  -err.- 43 0     43\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nUnlike in RQ 1 and 2, the algorithm now is predicted that everything will be 1 (i.e., that all dogs will perform better at the nonostensive task than the ostensive, which doesn't fit what we know about how dogs use human cueing). This is also not a very helpful algorithm to use in the future.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Random Forest\nrandomforest_predictions_RQ3 <- predict(random_forest_model_RQ_3, newdata = testing_data_RQ_3_factor)\n\n#Measure how well the model did at predictions\nperformance(randomforest_predictions_RQ3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mmce \n0.03546099 \n```\n\n\n:::\n\n```{.r .cell-code}\n#Dig deeper into predictions with a confusion matrix\ncalculateConfusionMatrix(randomforest_predictions_RQ3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        predicted\ntrue      0  1 -err.-\n  0      93  5      5\n  1       0 43      0\n  -err.-  0  5      5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThe random forest algorithm has the exact same results as the KNN.\n",
    "supporting": [
      "step5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}